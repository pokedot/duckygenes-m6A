M6ANET TRANSFORMER MIL RESULTS (IMPROVED - PERMUTATION INVARIANT)
================================================================================

Model: Transformer-based MIL (Permutation-Invariant Design)
Architecture:
  - 5 Transformer encoder layers
  - 4 attention heads in transformer
  - 4 attention heads in pooling layer
  - 128 embedding dimension
  - 256 feed-forward dimension


Permutation Invariance:
  - No positional encoding used
  - Permutation augmentation: True
  - Random shuffling of reads during training
  - Learned relationships are order-independent

MIL Characteristics:
  - Each site is a 'bag' of reads (instances)
  - Bag is positive if at least one read shows modification
  - Transformer learns complex relationships between reads
  - Multi-head attention provides interpretability

Model Configuration:
  Dropout: 0.3
  Focal loss: True
  Focal alpha: 0.75
  Focal gamma: 2.0

Training:
  Epochs trained: 50
  Batch size: 16
  Learning rate: 0.0001
  Weight decay: 1e-05
  Mixed precision: True
  Total parameters: 804,737

Data Split (by gene_id):
  Train: 84887 bags from 2696 genes (with augmentation)
  Valid: 19146 bags from 577 genes
  Test: 17805 bags from 579 genes

Test Set Results:
  Loss: 0.0162
  ROC-AUC: 0.9192
  PR-AUC: 0.4335
  Best F1: 0.4816  threshold 0.500

Best Validation PR-AUC: 0.5079

