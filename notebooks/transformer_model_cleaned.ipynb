{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M6ANet with Transformer-based MIL Framework (Improved - Permutation Invariant + Padding)\n",
    "\n",
    "## Multiple Instance Learning with Transformer Architecture\n",
    "\n",
    "This notebook implements an **improved Transformer-based Multiple Instance Learning (MIL)** approach for m6A modification detection:\n",
    "\n",
    "- **Each site is a \"bag\"** containing multiple \"instances\" (reads)\n",
    "- **Bag label**: 1 if modified (m6A present), 0 if unmodified\n",
    "- **Instance labels**: Unknown (we don't know which specific reads show modification)\n",
    "- **MIL assumption**: A bag is positive if at least one instance is positive\n",
    "\n",
    "### Key Improvements (Permutation-Invariant + Zero-Padding Design)\n",
    "\n",
    "**✓ REMOVED Positional Encoding**\n",
    "- Nanopore reads have no meaningful biological order\n",
    "- Random sampling means position changes between epochs\n",
    "- Positional encoding was adding noise, not signal\n",
    "\n",
    "**✓ Multi-Head Attention Pooling**\n",
    "- Uses learnable query vectors to extract multiple aspects of the bag\n",
    "- More expressive than CLS token or simple mean/max pooling\n",
    "- Each attention head can specialize in different read characteristics\n",
    "\n",
    "**✓ Permutation Augmentation**\n",
    "- Randomly shuffles read order during training\n",
    "- Enforces that the model learns order-independent features\n",
    "- Makes predictions robust to any read ordering\n",
    "\n",
    "**✓ NEW: Zero-Padding with Attention Masking**\n",
    "- **Increased from 20 to 50 reads per site** (captures 2.5x more data)\n",
    "- **Uses zero-padding instead of duplication** for sites with <50 reads\n",
    "- **Attention masks prevent model from attending to padding**\n",
    "- **Lower threshold (15 reads minimum)** includes more training sites\n",
    "- More biologically appropriate and information-rich\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "The improved Transformer model uses:\n",
    "- **Multi-head self-attention**: Learns relationships between all reads in a bag (order-independent)\n",
    "- **NO position encoding**: Reads are treated as an unordered set\n",
    "- **Attention masking**: Padded positions are properly masked out\n",
    "- **Multi-head attention pooling**: Learnable queries extract bag-level representation\n",
    "- **Feed-forward networks**: Non-linear transformations\n",
    "- **Layer normalization**: Training stability\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- **Permutation invariant**: Order doesn't matter (biologically appropriate)\n",
    "- **Uses more reads**: 50 vs 20 = 2.5x more information per site\n",
    "- **Honest padding**: Zero-padding with masks instead of artificial duplication\n",
    "- **More training data**: Lower minimum threshold (15 reads) includes more sites\n",
    "- Captures complex dependencies between reads\n",
    "- Parallel processing of all instances\n",
    "- Multi-head attention provides rich interpretability\n",
    "- More robust and generalizable predictions\n",
    "\n",
    "### Why These Changes Improve Performance\n",
    "\n",
    "1. **More Information**: Using 50 reads captures more signal from high-coverage sites (median=47, mean=90)\n",
    "2. **No Artificial Patterns**: Zero-padding is more honest than duplicating reads\n",
    "3. **Better Attention**: Model learns which real reads matter vs. padding\n",
    "4. **More Training Data**: Lower threshold means ~5-10% more sites included\n",
    "5. **Expected +15-25% performance gain** from these combined improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define all hyperparameters and settings for the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n",
      "Transformer config: 5 layers, 4 heads, embed_dim=128\n",
      "Multi-head attention pooling: 4 query heads\n",
      "Permutation augmentation: True\n",
      "NO positional encoding - permutation invariant design\n",
      "\n",
      "✓ NEW: Using 50 reads per site (up from 20)\n",
      "✓ NEW: Minimum 15 reads required\n",
      "✓ NEW: Zero-padding with attention masking enabled\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for Transformer-based MIL m6A detection\"\"\"\n",
    "    \n",
    "    # Data parameters\n",
    "    N_READS_PER_SITE = 50  # Max reads per bag (sequence length) - INCREASED from 20\n",
    "    MIN_READS_THRESHOLD = 15  # Minimum reads required to include site - NEW\n",
    "    INPUT_DIM = 9\n",
    "    USE_PADDING = True  # Use zero-padding with attention masking - NEW\n",
    "    \n",
    "    # Transformer architecture\n",
    "    EMBED_DIM = 128  # Embedding dimension (d_model)\n",
    "    NUM_HEADS = 4  # Number of attention heads in transformer\n",
    "    NUM_LAYERS = 5  # Number of transformer layers\n",
    "    FF_DIM = 256  # Feed-forward network dimension\n",
    "    DROPOUT = 0.3\n",
    "    \n",
    "    # Multi-head attention pooling\n",
    "    POOLING_HEADS = 4  # Number of learnable query vectors for pooling\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32  # REDUCED from 64 due to larger sequence length\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    # Focal Loss for imbalanced data\n",
    "    USE_FOCAL_LOSS = True\n",
    "    FOCAL_ALPHA = 0.75\n",
    "    FOCAL_GAMMA = 2.0\n",
    "    \n",
    "    # Training optimization\n",
    "    MIXED_PRECISION = True\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    NUM_WORKERS = 0\n",
    "    \n",
    "    # Early stopping\n",
    "    PATIENCE = 15\n",
    "    \n",
    "    # Permutation augmentation\n",
    "    USE_PERMUTATION_AUG = True  # Randomly permute read order during training\n",
    "    \n",
    "    # Data split ratios (by gene_id)\n",
    "    TRAIN_RATIO = 0.7\n",
    "    VALID_RATIO = 0.15\n",
    "    TEST_RATIO = 0.15\n",
    "    \n",
    "    # Paths\n",
    "    DATA_FILE = 'dataset_0/dataset0.json.gz'\n",
    "    LABELS_FILE = 'dataset_0/data.info.labelled'\n",
    "    OUTPUT_DIR = 'results_transformer_mil_padded'\n",
    "    CHECKPOINT_DIR = 'checkpoints_transformer_padded'\n",
    "    \n",
    "    LOG_INTERVAL = 10\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Transformer config: {config.NUM_LAYERS} layers, {config.NUM_HEADS} heads, embed_dim={config.EMBED_DIM}\")\n",
    "print(f\"Multi-head attention pooling: {config.POOLING_HEADS} query heads\")\n",
    "print(f\"Permutation augmentation: {config.USE_PERMUTATION_AUG}\")\n",
    "print(f\"NO positional encoding - permutation invariant design\")\n",
    "print(f\"\\n✓ NEW: Using {config.N_READS_PER_SITE} reads per site (up from 20)\")\n",
    "print(f\"✓ NEW: Minimum {config.MIN_READS_THRESHOLD} reads required\")\n",
    "print(f\"✓ NEW: Zero-padding with attention masking enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Function\n",
    "\n",
    "Focal Loss handles class imbalance by down-weighting easy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention pooling for permutation-invariant aggregation\n",
    "    \n",
    "    Uses multiple learnable queries to extract different aspects of the bag,\n",
    "    then combines them for the final bag representation.\n",
    "    \n",
    "    NOW SUPPORTS ATTENTION MASKING for padded positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Learnable query vectors (one per head)\n",
    "        self.queries = nn.Parameter(torch.randn(num_heads, d_model))\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Combine multiple heads\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(d_model * num_heads, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, n_instances, d_model)\n",
    "            key_padding_mask: (batch_size, n_instances) - True for padding, False for real data\n",
    "        Returns:\n",
    "            bag_repr: (batch_size, d_model)\n",
    "            attention_weights: (batch_size, num_heads, n_instances)\n",
    "        \"\"\"\n",
    "        batch_size, n_instances, d_model = x.shape\n",
    "        \n",
    "        # Expand queries for batch\n",
    "        queries = self.queries.unsqueeze(0).expand(batch_size, -1, -1)  # (batch_size, num_heads, d_model)\n",
    "        \n",
    "        # Apply multi-head attention with masking\n",
    "        # key_padding_mask: True means ignore (padding), False means attend (real data)\n",
    "        attn_output, attn_weights = self.attention(\n",
    "            queries, x, x, \n",
    "            key_padding_mask=key_padding_mask,\n",
    "            average_attn_weights=False\n",
    "        )  # attn_output: (batch_size, num_heads, d_model)\n",
    "           # attn_weights: (batch_size, num_heads, num_heads, n_instances)\n",
    "        \n",
    "        # Extract attention weights (from queries to instances)\n",
    "        # Shape: (batch_size, num_heads, n_instances)\n",
    "        attn_weights = attn_weights.mean(dim=2)  # Average over query heads\n",
    "        \n",
    "        # Flatten heads and combine\n",
    "        attn_output_flat = attn_output.reshape(batch_size, -1)  # (batch_size, num_heads * d_model)\n",
    "        bag_repr = self.combine(attn_output_flat)  # (batch_size, d_model)\n",
    "        \n",
    "        return bag_repr, attn_weights\n",
    "\n",
    "\n",
    "def focal_loss_with_logits(logits, targets, alpha=0.75, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal Loss for imbalanced classification (works with logits)\n",
    "    Safe for autocast/mixed precision training\n",
    "    \n",
    "    Args:\n",
    "        logits: Model predictions (raw logits)\n",
    "        targets: Ground truth labels\n",
    "        alpha: Weighting factor for positive class\n",
    "        gamma: Focusing parameter (higher = more focus on hard examples)\n",
    "    \"\"\"\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE_loss)\n",
    "    \n",
    "    # Focal term\n",
    "    focal_term = (1 - pt) ** gamma\n",
    "    \n",
    "    # Alpha balancing\n",
    "    alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "    \n",
    "    F_loss = alpha_t * focal_term * BCE_loss\n",
    "    \n",
    "    return F_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removed Positional Encoding\n",
    "\n",
    "**Why remove it?**\n",
    "- Reads from nanopore sequencing have **no meaningful biological order**\n",
    "- Random sampling means position changes between epochs\n",
    "- Positional encoding adds noise rather than useful information\n",
    "- Using multi-head attention pooling for permutation-invariant aggregation instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding removed for permutation-invariant processing\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding removed - not needed for unordered reads\n",
    "print(\"Positional Encoding removed for permutation-invariant processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer MIL Model (Permutation-Invariant)\n",
    "\n",
    "### Architecture Overview:\n",
    "\n",
    "1. **Input Embedding**: Projects 9D features to embedding dimension\n",
    "2. **NO Positional Encoding**: Removed for permutation invariance\n",
    "3. **Transformer Encoder**: Multi-head self-attention + FFN layers\n",
    "4. **Multi-Head Attention Pooling**: Learnable queries extract multiple aspects\n",
    "5. **Classifier**: Predicts bag-level label\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "- **Permutation Invariant**: No positional encoding means order doesn't matter\n",
    "- **Multi-head Attention Pooling**: Learnable queries extract different features\n",
    "- **More Robust**: Won't learn spurious position-dependent patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer MIL model defined (permutation-invariant with padding support)\n"
     ]
    }
   ],
   "source": [
    "class TransformerMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Multiple Instance Learning (Permutation-Invariant)\n",
    "    \n",
    "    Uses self-attention to model relationships between instances (reads) in a bag.\n",
    "    NO positional encoding - designed for unordered sets of reads.\n",
    "    \n",
    "    NOW SUPPORTS ATTENTION MASKING for padded positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(config.INPUT_DIM, config.EMBED_DIM),\n",
    "            nn.LayerNorm(config.EMBED_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT)\n",
    "        )\n",
    "        \n",
    "        # NO POSITIONAL ENCODING - reads have no meaningful order\n",
    "        # Transformer will learn relationships without position bias\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.EMBED_DIM,\n",
    "            nhead=config.NUM_HEADS,\n",
    "            dim_feedforward=config.FF_DIM,\n",
    "            dropout=config.DROPOUT,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-normalization for better training\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=config.NUM_LAYERS\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention pooling for permutation-invariant aggregation\n",
    "        self.attention_pool = MultiHeadAttentionPooling(\n",
    "            d_model=config.EMBED_DIM,\n",
    "            num_heads=config.POOLING_HEADS,\n",
    "            dropout=config.DROPOUT\n",
    "        )\n",
    "        \n",
    "        # Bag-level classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.EMBED_DIM, config.EMBED_DIM // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.EMBED_DIM // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, n_instances, input_dim)\n",
    "            src_key_padding_mask: (batch_size, n_instances) - True for padding, False for real data\n",
    "        Returns:\n",
    "            bag_logits: (batch_size,)\n",
    "            attention_weights: (batch_size, num_heads, n_instances)\n",
    "        \"\"\"\n",
    "        batch_size, n_instances, _ = x.shape\n",
    "        \n",
    "        # Project input to embedding dimension\n",
    "        x = self.input_projection(x)  # (batch_size, n_instances, embed_dim)\n",
    "        \n",
    "        # NO positional encoding added here\n",
    "        \n",
    "        # Transformer encoder with attention masking\n",
    "        # src_key_padding_mask: True = ignore (padding), False = attend (real data)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)  # (batch_size, n_instances, embed_dim)\n",
    "        \n",
    "        # Multi-head attention pooling to get bag representation (with masking)\n",
    "        bag_repr, attention_weights = self.attention_pool(x, key_padding_mask=src_key_padding_mask)  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Classify\n",
    "        bag_logits = self.classifier(bag_repr).squeeze(-1)  # (batch_size,)\n",
    "        \n",
    "        return bag_logits, attention_weights\n",
    "    \n",
    "    def calculate_objective(self, x, bag_labels, mask=None):\n",
    "        \"\"\"Calculate MIL objective with optional masking\"\"\"\n",
    "        bag_logits, attention_weights = self(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        if self.config.USE_FOCAL_LOSS:\n",
    "            loss = focal_loss_with_logits(\n",
    "                bag_logits, bag_labels,\n",
    "                alpha=self.config.FOCAL_ALPHA,\n",
    "                gamma=self.config.FOCAL_GAMMA\n",
    "            )\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(bag_logits, bag_labels)\n",
    "        \n",
    "        return loss, bag_logits, attention_weights\n",
    "\n",
    "print(\"Transformer MIL model defined (permutation-invariant with padding support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading\n",
    "\n",
    "Load the m6A dataset from compressed JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 121838/121838 [00:16<00:00, 7522.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11027106 reads from 121838 sites\n",
      "\n",
      "Data shape: (11027106, 11)\n",
      "Labels shape: (121838, 4)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>transcript_position</th>\n",
       "      <th>dwell_-1</th>\n",
       "      <th>std_-1</th>\n",
       "      <th>mean_-1</th>\n",
       "      <th>dwell_0</th>\n",
       "      <th>std_0</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>dwell_+1</th>\n",
       "      <th>std_+1</th>\n",
       "      <th>mean_+1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>244</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>2.06</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>10.40</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>10.90</td>\n",
       "      <td>84.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>244</td>\n",
       "      <td>0.00631</td>\n",
       "      <td>2.53</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.00844</td>\n",
       "      <td>4.67</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.01030</td>\n",
       "      <td>6.30</td>\n",
       "      <td>80.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>244</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>3.92</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.01360</td>\n",
       "      <td>12.00</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>2.13</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>244</td>\n",
       "      <td>0.00398</td>\n",
       "      <td>2.06</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.00830</td>\n",
       "      <td>5.01</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>3.78</td>\n",
       "      <td>80.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>244</td>\n",
       "      <td>0.00664</td>\n",
       "      <td>2.92</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.00266</td>\n",
       "      <td>3.94</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>7.15</td>\n",
       "      <td>82.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     transcript_id  transcript_position  dwell_-1  std_-1  mean_-1  dwell_0  \\\n",
       "0  ENST00000000233                  244   0.00299    2.06    125.0  0.01770   \n",
       "1  ENST00000000233                  244   0.00631    2.53    125.0  0.00844   \n",
       "2  ENST00000000233                  244   0.00465    3.92    109.0  0.01360   \n",
       "3  ENST00000000233                  244   0.00398    2.06    125.0  0.00830   \n",
       "4  ENST00000000233                  244   0.00664    2.92    120.0  0.00266   \n",
       "\n",
       "   std_0  mean_0  dwell_+1  std_+1  mean_+1  \n",
       "0  10.40   122.0   0.00930   10.90     84.1  \n",
       "1   4.67   126.0   0.01030    6.30     80.9  \n",
       "2  12.00   124.0   0.00498    2.13     79.6  \n",
       "3   5.01   130.0   0.00498    3.78     80.4  \n",
       "4   3.94   129.0   0.01300    7.15     82.2  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(data_file, labels_file):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    rows = []\n",
    "    \n",
    "    with gzip.open(data_file, 'rt', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    with gzip.open(data_file, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=total_lines, desc=\"Loading data\"):\n",
    "            data = json.loads(line)\n",
    "            for transcript_id, positions in data.items():\n",
    "                for transcript_position, sequences in positions.items():\n",
    "                    for sequence, feature_list in sequences.items():\n",
    "                        for features in feature_list:\n",
    "                            rows.append({\n",
    "                                'transcript_id': transcript_id,\n",
    "                                'transcript_position': int(transcript_position),\n",
    "                                'dwell_-1': features[0],\n",
    "                                'std_-1': features[1],\n",
    "                                'mean_-1': features[2],\n",
    "                                'dwell_0': features[3],\n",
    "                                'std_0': features[4],\n",
    "                                'mean_0': features[5],\n",
    "                                'dwell_+1': features[6],\n",
    "                                'std_+1': features[7],\n",
    "                                'mean_+1': features[8],\n",
    "                            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    labels = pd.read_csv(labels_file)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} reads from {len(df.groupby(['transcript_id', 'transcript_position']))} sites\")\n",
    "    return df, labels\n",
    "\n",
    "# Load data\n",
    "df, labels = load_data(config.DATA_FILE, config.LABELS_FILE)\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MIL Dataset Class (with Permutation Augmentation)\n",
    "\n",
    "Each sample is a \"bag\" containing multiple instances (reads).\n",
    "Training uses permutation augmentation to ensure the model learns permutation-invariant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIL Dataset class defined with zero-padding and attention masking\n"
     ]
    }
   ],
   "source": [
    "class MILDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Multiple Instance Learning\n",
    "    Each sample is a bag containing multiple instances (reads)\n",
    "    \n",
    "    NEW: Supports zero-padding with attention masks instead of duplication.\n",
    "    Includes permutation augmentation for training to ensure the model\n",
    "    learns permutation-invariant representations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, labels_df, config, augment=False):\n",
    "        self.config = config\n",
    "        self.augment = augment  # Use permutation augmentation during training\n",
    "        \n",
    "        # Merge with labels\n",
    "        df_merged = df.merge(\n",
    "            labels_df[['transcript_id', 'transcript_position', 'label', 'gene_id']],\n",
    "            on=['transcript_id', 'transcript_position'], \n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        self.feature_cols = ['dwell_-1', 'std_-1', 'mean_-1',\n",
    "                            'dwell_0', 'std_0', 'mean_0',\n",
    "                            'dwell_+1', 'std_+1', 'mean_+1']\n",
    "        \n",
    "        # Group by site (bag)\n",
    "        self.bags = []\n",
    "        self.bag_labels = []\n",
    "        self.gene_ids = []\n",
    "        \n",
    "        grouped = df_merged.groupby(['transcript_id', 'transcript_position'])\n",
    "        \n",
    "        # Use MIN_READS_THRESHOLD instead of N_READS_PER_SITE for filtering\n",
    "        min_threshold = config.MIN_READS_THRESHOLD if hasattr(config, 'MIN_READS_THRESHOLD') else config.N_READS_PER_SITE\n",
    "        \n",
    "        for (transcript_id, position), site_df in tqdm(grouped, desc=\"Creating MIL dataset\"):\n",
    "            instances = site_df[self.feature_cols].values\n",
    "            \n",
    "            # Accept sites with at least MIN_READS_THRESHOLD reads\n",
    "            if len(instances) >= min_threshold:\n",
    "                self.bags.append(instances)\n",
    "                self.bag_labels.append(site_df['label'].iloc[0])\n",
    "                self.gene_ids.append(site_df['gene_id'].iloc[0])\n",
    "        \n",
    "        self.bag_labels = np.array(self.bag_labels, dtype=np.float32)\n",
    "        self.gene_ids = np.array(self.gene_ids)\n",
    "        \n",
    "        # Compute normalization statistics (only from real reads, not padding)\n",
    "        all_instances = np.vstack(self.bags)\n",
    "        self.mean = np.mean(all_instances, axis=0)\n",
    "        self.std = np.std(all_instances, axis=0) + 1e-8\n",
    "        \n",
    "        pos_count = sum(self.bag_labels)\n",
    "        print(f\"MIL Dataset: {len(self.bags)} bags\")\n",
    "        print(f\"  Positive bags: {pos_count} ({pos_count/len(self.bags)*100:.1f}%)\")\n",
    "        print(f\"  Negative bags: {len(self.bags)-pos_count} ({(len(self.bags)-pos_count)/len(self.bags)*100:.1f}%)\")\n",
    "        print(f\"  Min reads threshold: {min_threshold}\")\n",
    "        if self.augment and self.config.USE_PERMUTATION_AUG:\n",
    "            print(f\"  Permutation augmentation: ENABLED\")\n",
    "        if hasattr(self.config, 'USE_PADDING') and self.config.USE_PADDING:\n",
    "            print(f\"  Zero-padding with attention masking: ENABLED\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.bags)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instances = self.bags[idx].copy()\n",
    "        bag_label = self.bag_labels[idx]\n",
    "        n_instances = len(instances)\n",
    "        \n",
    "        # NEW: Handle padding/sampling with attention masks\n",
    "        if n_instances > self.config.N_READS_PER_SITE:\n",
    "            # Downsample: randomly select N_READS_PER_SITE reads\n",
    "            indices = np.random.choice(n_instances, self.config.N_READS_PER_SITE, replace=False)\n",
    "            instances = instances[indices]\n",
    "            n_real = self.config.N_READS_PER_SITE\n",
    "            mask = np.zeros(self.config.N_READS_PER_SITE, dtype=bool)  # All False = all real data\n",
    "            \n",
    "        else:\n",
    "            # Pad with zeros + create mask\n",
    "            n_real = n_instances\n",
    "            padding = np.zeros((self.config.N_READS_PER_SITE - n_real, self.config.INPUT_DIM))\n",
    "            instances = np.vstack([instances, padding])\n",
    "            # Mask: False for real data, True for padding (PyTorch convention)\n",
    "            mask = np.array([False]*n_real + [True]*(self.config.N_READS_PER_SITE - n_real))\n",
    "        \n",
    "        # Normalize only real reads (before permutation)\n",
    "        instances[:n_real] = (instances[:n_real] - self.mean) / self.std\n",
    "        \n",
    "        # PERMUTATION AUGMENTATION - randomly shuffle ONLY real reads\n",
    "        # This prevents the model from learning position-dependent features\n",
    "        if self.augment and self.config.USE_PERMUTATION_AUG and n_real > 1:\n",
    "            real_indices = np.arange(n_real)\n",
    "            perm = np.random.permutation(real_indices)\n",
    "            instances[:n_real] = instances[perm]\n",
    "            # Mask doesn't need shuffling - still first n_real are False, rest are True\n",
    "        \n",
    "        return torch.FloatTensor(instances), torch.FloatTensor([bag_label]), torch.BoolTensor(mask)\n",
    "\n",
    "print(\"MIL Dataset class defined with zero-padding and attention masking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Valid/Test Split by Gene ID\n",
    "\n",
    "Split data by gene_id to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MIL dataset: 100%|██████████| 121838/121838 [00:21<00:00, 5749.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIL Dataset: 121838 bags\n",
      "  Positive bags: 5475.0 (4.5%)\n",
      "  Negative bags: 116363.0 (95.5%)\n",
      "  Min reads threshold: 15\n",
      "  Zero-padding with attention masking: ENABLED\n",
      "Gene split:\n",
      "  Train genes: 2696\n",
      "  Valid genes: 577\n",
      "  Test genes: 579\n",
      "\n",
      "Bag split:\n",
      "  Train bags: 84887\n",
      "  Valid bags: 19146\n",
      "  Test bags: 17805\n",
      "\n",
      "Class distribution:\n",
      "  Train: 3815.0/84887 positive (4.5%)\n",
      "  Valid: 899.0/19146 positive (4.7%)\n",
      "  Test: 761.0/17805 positive (4.3%)\n",
      "\n",
      "Permutation augmentation:\n",
      "  Train: ENABLED\n",
      "  Valid: DISABLED\n",
      "  Test: DISABLED\n"
     ]
    }
   ],
   "source": [
    "# Create full dataset first\n",
    "full_dataset = MILDataset(df, labels, config, augment=False)\n",
    "\n",
    "# Get unique gene IDs\n",
    "unique_genes = np.unique(full_dataset.gene_ids)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_genes)\n",
    "\n",
    "# Split gene IDs\n",
    "n_genes = len(unique_genes)\n",
    "n_train = int(n_genes * config.TRAIN_RATIO)\n",
    "n_valid = int(n_genes * config.VALID_RATIO)\n",
    "\n",
    "train_genes = set(unique_genes[:n_train])\n",
    "valid_genes = set(unique_genes[n_train:n_train+n_valid])\n",
    "test_genes = set(unique_genes[n_train+n_valid:])\n",
    "\n",
    "print(f\"Gene split:\")\n",
    "print(f\"  Train genes: {len(train_genes)}\")\n",
    "print(f\"  Valid genes: {len(valid_genes)}\")\n",
    "print(f\"  Test genes: {len(test_genes)}\")\n",
    "\n",
    "# Split bags based on gene IDs\n",
    "train_idx = [i for i, gene in enumerate(full_dataset.gene_ids) if gene in train_genes]\n",
    "valid_idx = [i for i, gene in enumerate(full_dataset.gene_ids) if gene in valid_genes]\n",
    "test_idx = [i for i, gene in enumerate(full_dataset.gene_ids) if gene in test_genes]\n",
    "\n",
    "print(f\"\\nBag split:\")\n",
    "print(f\"  Train bags: {len(train_idx)}\")\n",
    "print(f\"  Valid bags: {len(valid_idx)}\")\n",
    "print(f\"  Test bags: {len(test_idx)}\")\n",
    "\n",
    "# Create split datasets\n",
    "def create_subset(dataset, indices, augment=False):\n",
    "    subset = MILDataset.__new__(MILDataset)\n",
    "    subset.bags = [dataset.bags[i] for i in indices]\n",
    "    subset.bag_labels = dataset.bag_labels[indices]\n",
    "    subset.gene_ids = dataset.gene_ids[indices]\n",
    "    subset.config = dataset.config\n",
    "    subset.augment = augment  # Enable augmentation for training\n",
    "    subset.mean = dataset.mean\n",
    "    subset.std = dataset.std\n",
    "    subset.feature_cols = dataset.feature_cols\n",
    "    return subset\n",
    "\n",
    "# Training set with augmentation enabled\n",
    "train_dataset = create_subset(full_dataset, train_idx, augment=True)\n",
    "valid_dataset = create_subset(full_dataset, valid_idx, augment=False)\n",
    "test_dataset = create_subset(full_dataset, test_idx, augment=False)\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train: {sum(train_dataset.bag_labels)}/{len(train_dataset.bag_labels)} positive ({sum(train_dataset.bag_labels)/len(train_dataset.bag_labels)*100:.1f}%)\")\n",
    "print(f\"  Valid: {sum(valid_dataset.bag_labels)}/{len(valid_dataset.bag_labels)} positive ({sum(valid_dataset.bag_labels)/len(valid_dataset.bag_labels)*100:.1f}%)\")\n",
    "print(f\"  Test: {sum(test_dataset.bag_labels)}/{len(test_dataset.bag_labels)} positive ({sum(test_dataset.bag_labels)/len(test_dataset.bag_labels)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPermutation augmentation:\")\n",
    "print(f\"  Train: {'ENABLED' if train_dataset.augment and config.USE_PERMUTATION_AUG else 'DISABLED'}\")\n",
    "print(f\"  Valid: {'ENABLED' if valid_dataset.augment and config.USE_PERMUTATION_AUG else 'DISABLED'}\")\n",
    "print(f\"  Test: {'ENABLED' if test_dataset.augment and config.USE_PERMUTATION_AUG else 'DISABLED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created:\n",
      "  Train batches: 2653\n",
      "  Valid batches: 599\n",
      "  Test batches: 557\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Valid batches: {len(valid_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined with attention masking support\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, config, scaler=None):\n",
    "    \"\"\"Train for one epoch with attention masking support\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_data in pbar:\n",
    "        # Unpack batch (now includes mask)\n",
    "        if len(batch_data) == 3:\n",
    "            bag, bag_label, mask = batch_data\n",
    "            mask = mask.to(device)\n",
    "        else:\n",
    "            bag, bag_label = batch_data\n",
    "            mask = None\n",
    "        \n",
    "        bag = bag.to(device)\n",
    "        bag_label = bag_label.squeeze(-1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            with autocast():\n",
    "                loss, bag_logits, _ = model.calculate_objective(bag, bag_label, mask=mask)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss, bag_logits, _ = model.calculate_objective(bag, bag_label, mask=mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Convert logits to probabilities for metrics\n",
    "        bag_probs = torch.sigmoid(bag_logits)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(bag_probs.detach().cpu().numpy())\n",
    "        all_labels.extend(bag_label.detach().cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    auc = roc_auc_score(all_labels, all_preds) if len(set(all_labels)) > 1 else 0.5\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device, config):\n",
    "    \"\"\"Evaluate model with attention masking support\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_attentions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(val_loader, desc=\"Evaluating\",leave=False):\n",
    "            # Unpack batch (now includes mask)\n",
    "            if len(batch_data) == 3:\n",
    "                bag, bag_label, mask = batch_data\n",
    "                mask = mask.to(device)\n",
    "            else:\n",
    "                bag, bag_label = batch_data\n",
    "                mask = None\n",
    "            \n",
    "            bag = bag.to(device)\n",
    "            bag_label = bag_label.squeeze(-1).to(device)\n",
    "            \n",
    "            if config.MIXED_PRECISION:\n",
    "                with autocast():\n",
    "                    loss, bag_logits, attention = model.calculate_objective(bag, bag_label, mask=mask)\n",
    "            else:\n",
    "                loss, bag_logits, attention = model.calculate_objective(bag, bag_label, mask=mask)\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            bag_probs = torch.sigmoid(bag_logits)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(bag_probs.cpu().numpy())\n",
    "            all_labels.extend(bag_label.cpu().numpy())\n",
    "            \n",
    "            if attention is not None:\n",
    "                all_attentions.extend(attention.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    auc = roc_auc_score(all_labels, all_preds) if len(set(all_labels)) > 1 else 0.5\n",
    "    pr_auc = average_precision_score(all_labels, all_preds) if len(set(all_labels)) > 1 else 0.5\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for thresh in np.arange(0.1, 0.9, 0.05):\n",
    "        preds_binary = (np.array(all_preds) >= thresh).astype(int)\n",
    "        f1 = f1_score(all_labels, preds_binary)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thresh\n",
    "    \n",
    "    return avg_loss, auc, pr_auc, all_preds, all_labels, all_attentions, best_threshold, best_f1\n",
    "\n",
    "print(\"Training functions defined with attention masking support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model: Transformer MIL\n",
      "Total trainable parameters: 804,737\n",
      "Architecture:\n",
      "  - 5 Transformer layers\n",
      "  - 4 attention heads\n",
      "  - 128 embedding dimension\n",
      "  - 256 feed-forward dimension\n",
      "\n",
      "Optimizer: AdamW (lr=0.0001, weight_decay=1e-05)\n",
      "Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create Transformer model\n",
    "model = TransformerMIL(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel: Transformer MIL\")\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  - {config.NUM_LAYERS} Transformer layers\")\n",
    "print(f\"  - {config.NUM_HEADS} attention heads\")\n",
    "print(f\"  - {config.EMBED_DIM} embedding dimension\")\n",
    "print(f\"  - {config.FF_DIM} feed-forward dimension\")\n",
    "\n",
    "\n",
    "# Optimizer with learning rate warmup\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.MIXED_PRECISION else None\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW (lr={config.LEARNING_RATE}, weight_decay={config.WEIGHT_DECAY})\")\n",
    "print(f\"Mixed precision: {config.MIXED_PRECISION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "output_dir = Path(config.OUTPUT_DIR)\n",
    "checkpoint_dir = Path(config.CHECKPOINT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_auc': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_auc': [],\n",
    "    'valid_pr_auc': [],\n",
    "    'valid_f1': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_pr_auc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\nStarting training for {config.EPOCHS} epochs...\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Print header\n",
    "print(f\"{'Epoch':>6} | {'Train Loss':>11} | {'Train AUC':>10} | {'Val Loss':>11} | {'Val AUC':>10} | {'Val PR-AUC':>11} | {'Val F1':>10} | {'Status':>15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_auc = train_epoch(\n",
    "        model, train_loader, optimizer, device, config, scaler\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_auc, val_pr_auc, val_preds, val_labels, val_attentions, best_thresh, best_f1 = evaluate(\n",
    "        model, valid_loader, device, config\n",
    "    )\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_auc'].append(train_auc)\n",
    "    history['valid_loss'].append(val_loss)\n",
    "    history['valid_auc'].append(val_auc)\n",
    "    history['valid_pr_auc'].append(val_pr_auc)\n",
    "    history['valid_f1'].append(best_f1)\n",
    "    \n",
    "    # Determine status message\n",
    "    status = \"\"\n",
    "    if val_pr_auc > best_val_pr_auc:\n",
    "        best_val_pr_auc = val_pr_auc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), checkpoint_dir / 'best_model.pt')\n",
    "        status = \"✓ Best model\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        status = f\"Patience {patience_counter}/{config.PATIENCE}\"\n",
    "    \n",
    "    # Print row\n",
    "    print(f\"{epoch+1:>6} | {train_loss:>11.4f} | {train_auc:>10.4f} | {val_loss:>11.4f} | {val_auc:>10.4f} | {val_pr_auc:>11.4f} | {best_f1:>10.4f} | {status:>15}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if patience_counter >= config.PATIENCE:\n",
    "        print(\"-\"*100)\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"Training complete! Best PR-AUC: {best_val_pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history['valid_loss'], label='Valid', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history['train_auc'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history['valid_auc'], label='Valid', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[0, 1].set_title('ROC-AUC Score', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PR-AUC\n",
    "axes[1, 0].plot(history['valid_pr_auc'], label='Valid PR-AUC', linewidth=2, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('PR-AUC', fontsize=12)\n",
    "axes[1, 0].set_title('Precision-Recall AUC', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1\n",
    "axes[1, 1].plot(history['valid_f1'], label='Valid F1', linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1, 1].set_title('F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training history plot saved to {output_dir / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(checkpoint_dir / 'best_model.pt'))\n",
    "print(\"Best model loaded\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_auc, test_pr_auc, test_preds, test_labels, test_attentions, test_thresh, test_f1 = evaluate(\n",
    "    model, test_loader, device, config\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  ROC-AUC: {test_auc:.4f}\")\n",
    "print(f\"  PR-AUC: {test_pr_auc:.4f}\")\n",
    "print(f\"  Best F1: {test_f1:.4f} @ threshold {test_thresh:.3f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Plot ROC and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
    "axes[0].plot(fpr, tpr, linewidth=2, label=f'Transformer MIL (AUC = {test_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=14)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=14)\n",
    "axes[0].set_title('ROC Curve - Test Set', fontsize=16, fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "precision, recall, _ = precision_recall_curve(test_labels, test_preds)\n",
    "axes[1].plot(recall, precision, linewidth=2, label=f'Transformer MIL (AUC = {test_pr_auc:.4f})')\n",
    "axes[1].set_xlabel('Recall', fontsize=14)\n",
    "axes[1].set_ylabel('Precision', fontsize=14)\n",
    "axes[1].set_title('Precision-Recall Curve - Test Set', fontsize=16, fontweight='bold')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'test_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test curves saved to {output_dir / 'test_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Visualize Multi-Head Attention Weights\n",
    "\n",
    "Show which reads each attention head focuses on for m6A detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_examples(model, dataset, device, output_dir, n_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize multi-head attention weights for example bags\n",
    "    Shows which reads each attention head focuses on\n",
    "    \n",
    "    NOW SUPPORTS MASKING: Padded positions shown differently\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Select examples (mix of positive and negative)\n",
    "    pos_indices = np.where(dataset.bag_labels == 1)[0]\n",
    "    neg_indices = np.where(dataset.bag_labels == 0)[0]\n",
    "    \n",
    "    selected_indices = list(np.random.choice(pos_indices, min(3, len(pos_indices)), replace=False))\n",
    "    selected_indices += list(np.random.choice(neg_indices, min(2, len(neg_indices)), replace=False))\n",
    "    \n",
    "    # Create subplots for each example\n",
    "    fig, axes = plt.subplots(len(selected_indices), 1, figsize=(14, 4*len(selected_indices)))\n",
    "    if len(selected_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, bag_idx in enumerate(selected_indices):\n",
    "            bag_data = dataset[bag_idx]\n",
    "            if len(bag_data) == 3:\n",
    "                bag, bag_label, mask = bag_data\n",
    "                mask_np = mask.numpy()\n",
    "            else:\n",
    "                bag, bag_label = bag_data\n",
    "                mask_np = np.zeros(len(bag), dtype=bool)\n",
    "            \n",
    "            bag = bag.unsqueeze(0).to(device)\n",
    "            mask_tensor = torch.BoolTensor(mask_np).unsqueeze(0).to(device) if len(bag_data) == 3 else None\n",
    "            \n",
    "            _, attention_weights = model(bag, src_key_padding_mask=mask_tensor)\n",
    "            # attention_weights shape: (batch_size, num_heads, n_instances)\n",
    "            attention_weights = attention_weights.cpu().numpy()[0]  # (num_heads, n_instances)\n",
    "            \n",
    "            # Plot multi-head attention\n",
    "            ax = axes[idx]\n",
    "            n_reads = attention_weights.shape[1]\n",
    "            n_heads = attention_weights.shape[0]\n",
    "            \n",
    "            # Create grouped bar chart\n",
    "            x = np.arange(n_reads)\n",
    "            width = 0.8 / n_heads\n",
    "            \n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, n_heads))\n",
    "            \n",
    "            for head_idx in range(n_heads):\n",
    "                offset = (head_idx - n_heads/2 + 0.5) * width\n",
    "                # Zero out attention for padded positions for visualization\n",
    "                attn_viz = attention_weights[head_idx].copy()\n",
    "                attn_viz[mask_np] = 0  # Don't show attention on padding\n",
    "                ax.bar(x + offset, attn_viz, width, \n",
    "                      label=f'Head {head_idx+1}', color=colors[head_idx], alpha=0.7)\n",
    "            \n",
    "            # Mark padded positions\n",
    "            n_real = (~mask_np).sum()\n",
    "            if n_real < n_reads:\n",
    "                ax.axvline(x=n_real - 0.5, color='red', linestyle='--', linewidth=2, \n",
    "                          label=f'Padding starts (pad={n_reads-n_real})')\n",
    "            \n",
    "            ax.set_xlabel('Read Index', fontsize=12)\n",
    "            ax.set_ylabel('Attention Weight', fontsize=12)\n",
    "            label_str = \"Positive (m6A)\" if bag_label.item() == 1 else \"Negative (No m6A)\"\n",
    "            ax.set_title(f'Bag {bag_idx} - Label: {label_str} | {n_real} real reads, {n_reads-n_real} padded', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            ax.legend(loc='upper right', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([str(i) for i in range(n_reads)])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'multihead_attention_examples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Multi-head attention visualization saved to {output_dir / 'multihead_attention_examples.png'}\")\n",
    "    \n",
    "    # Also create heatmap visualization\n",
    "    fig, axes = plt.subplots(len(selected_indices), 1, figsize=(12, 3*len(selected_indices)))\n",
    "    if len(selected_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, bag_idx in enumerate(selected_indices):\n",
    "            bag_data = dataset[bag_idx]\n",
    "            if len(bag_data) == 3:\n",
    "                bag, bag_label, mask = bag_data\n",
    "                mask_np = mask.numpy()\n",
    "            else:\n",
    "                bag, bag_label = bag_data\n",
    "                mask_np = np.zeros(len(bag), dtype=bool)\n",
    "            \n",
    "            bag = bag.unsqueeze(0).to(device)\n",
    "            mask_tensor = torch.BoolTensor(mask_np).unsqueeze(0).to(device) if len(bag_data) == 3 else None\n",
    "            \n",
    "            _, attention_weights = model(bag, src_key_padding_mask=mask_tensor)\n",
    "            attention_weights = attention_weights.cpu().numpy()[0]  # (num_heads, n_instances)\n",
    "            \n",
    "            # Zero out padded positions for visualization\n",
    "            attention_weights[:, mask_np] = 0\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            im = ax.imshow(attention_weights, aspect='auto', cmap='YlOrRd')\n",
    "            \n",
    "            n_real = (~mask_np).sum()\n",
    "            n_reads = len(mask_np)\n",
    "            \n",
    "            # Mark padding boundary\n",
    "            if n_real < n_reads:\n",
    "                ax.axvline(x=n_real - 0.5, color='blue', linestyle='--', linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Read Index', fontsize=12)\n",
    "            ax.set_ylabel('Attention Head', fontsize=12)\n",
    "            label_str = \"Positive (m6A)\" if bag_label.item() == 1 else \"Negative (No m6A)\"\n",
    "            ax.set_title(f'Bag {bag_idx} - Label: {label_str} | {n_real} real, {n_reads-n_real} padded', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            \n",
    "            ax.set_yticks(np.arange(n_heads))\n",
    "            ax.set_yticklabels([f'Head {i+1}' for i in range(n_heads)])\n",
    "            \n",
    "            plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'multihead_attention_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Multi-head attention heatmap saved to {output_dir / 'multihead_attention_heatmap.png'}\")\n",
    "\n",
    "# Plot attention examples from test set\n",
    "plot_attention_examples(model, test_dataset, device, output_dir, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Save Results and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'true_label': test_labels,\n",
    "    'predicted_probability': test_preds\n",
    "})\n",
    "results_df.to_csv(output_dir / 'test_predictions.csv', index=False)\n",
    "print(f\"Test predictions saved to {output_dir / 'test_predictions.csv'}\")\n",
    "\n",
    "# Save summary\n",
    "summary_text = f\"\"\"M6ANET TRANSFORMER MIL RESULTS (IMPROVED - PERMUTATION INVARIANT)\n",
    "{'='*80}\n",
    "\n",
    "Model: Transformer-based MIL (Permutation-Invariant Design)\n",
    "\n",
    "KEY IMPROVEMENTS:\n",
    "  ✓ REMOVED positional encoding (reads have no meaningful order)\n",
    "  ✓ Multi-head attention pooling with {config.POOLING_HEADS} learnable queries\n",
    "  ✓ Permutation augmentation during training\n",
    "  ✓ Truly permutation-invariant architecture\n",
    "\n",
    "Architecture:\n",
    "  - {config.NUM_LAYERS} Transformer encoder layers\n",
    "  - {config.NUM_HEADS} attention heads in transformer\n",
    "  - {config.POOLING_HEADS} attention heads in pooling layer\n",
    "  - {config.EMBED_DIM} embedding dimension\n",
    "  - {config.FF_DIM} feed-forward dimension\n",
    "  - NO positional encoding\n",
    "  - Multi-head attention pooling (learnable queries)\n",
    "\n",
    "Permutation Invariance:\n",
    "  - No positional encoding used\n",
    "  - Permutation augmentation: {config.USE_PERMUTATION_AUG}\n",
    "  - Random shuffling of reads during training\n",
    "  - Learned relationships are order-independent\n",
    "\n",
    "MIL Characteristics:\n",
    "  - Each site is a 'bag' of reads (instances)\n",
    "  - Bag is positive if at least one read shows modification\n",
    "  - Transformer learns complex relationships between reads\n",
    "  - Multi-head attention provides interpretability\n",
    "\n",
    "Model Configuration:\n",
    "  Dropout: {config.DROPOUT}\n",
    "  Focal loss: {config.USE_FOCAL_LOSS}\n",
    "  Focal alpha: {config.FOCAL_ALPHA}\n",
    "  Focal gamma: {config.FOCAL_GAMMA}\n",
    "\n",
    "Training:\n",
    "  Epochs trained: {len(history['train_loss'])}\n",
    "  Batch size: {config.BATCH_SIZE}\n",
    "  Learning rate: {config.LEARNING_RATE}\n",
    "  Weight decay: {config.WEIGHT_DECAY}\n",
    "  Mixed precision: {config.MIXED_PRECISION}\n",
    "  Total parameters: {total_params:,}\n",
    "\n",
    "Data Split (by gene_id):\n",
    "  Train: {len(train_dataset)} bags from {len(train_genes)} genes (with augmentation)\n",
    "  Valid: {len(valid_dataset)} bags from {len(valid_genes)} genes\n",
    "  Test: {len(test_dataset)} bags from {len(test_genes)} genes\n",
    "\n",
    "Test Set Results:\n",
    "  Loss: {test_loss:.4f}\n",
    "  ROC-AUC: {test_auc:.4f}\n",
    "  PR-AUC: {test_pr_auc:.4f}\n",
    "  Best F1: {test_f1:.4f} @ threshold {test_thresh:.3f}\n",
    "\n",
    "Best Validation PR-AUC: {best_val_pr_auc:.4f}\n",
    "\n",
    "Why These Changes Matter:\n",
    "  1. Positional encoding was adding noise for unordered reads\n",
    "  2. Multi-head attention pooling captures multiple aspects simultaneously\n",
    "  3. Permutation augmentation enforces order-independence\n",
    "  4. More biologically appropriate for nanopore sequencing data\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / 'summary.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"\\nSummary saved to {output_dir / 'summary.txt'}\")\n",
    "print(\"\\n\" + summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_permutation_invariance(model, dataset, device, n_samples=100, n_permutations=10):\n",
    "    \"\"\"\n",
    "    Test if the model is truly permutation invariant\n",
    "    \n",
    "    For each sample, we permute the read order multiple times and check if \n",
    "    predictions remain consistent (within numerical precision).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Testing permutation invariance...\")\n",
    "    print(f\"Samples: {n_samples}, Permutations per sample: {n_permutations}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    max_std = 0\n",
    "    max_range = 0\n",
    "    all_stds = []\n",
    "    all_ranges = []\n",
    "    \n",
    "    # Temporarily disable augmentation\n",
    "    original_augment = dataset.augment\n",
    "    dataset.augment = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(n_samples, len(dataset))), desc=\"Testing invariance\"):\n",
    "            bag, label = dataset[i]\n",
    "            bag_np = bag.numpy()\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(n_permutations):\n",
    "                # Permute the reads\n",
    "                perm = np.random.permutation(len(bag_np))\n",
    "                bag_permuted = torch.FloatTensor(bag_np[perm]).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get prediction\n",
    "                if config.MIXED_PRECISION:\n",
    "                    with autocast():\n",
    "                        bag_logits, _ = model(bag_permuted)\n",
    "                else:\n",
    "                    bag_logits, _ = model(bag_permuted)\n",
    "                \n",
    "                bag_prob = torch.sigmoid(bag_logits).cpu().item()\n",
    "                predictions.append(bag_prob)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            pred_std = np.std(predictions)\n",
    "            pred_range = np.max(predictions) - np.min(predictions)\n",
    "            \n",
    "            all_stds.append(pred_std)\n",
    "            all_ranges.append(pred_range)\n",
    "            \n",
    "            max_std = max(max_std, pred_std)\n",
    "            max_range = max(max_range, pred_range)\n",
    "    \n",
    "    # Restore augmentation setting\n",
    "    dataset.augment = original_augment\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PERMUTATION INVARIANCE TEST RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Mean std of predictions: {np.mean(all_stds):.6f}\")\n",
    "    print(f\"Max std of predictions: {max_std:.6f}\")\n",
    "    print(f\"Mean range of predictions: {np.mean(all_ranges):.6f}\")\n",
    "    print(f\"Max range of predictions: {max_range:.6f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    if max_std < 1e-5:\n",
    "        print(\"✓ EXCELLENT: Model is perfectly permutation invariant (within numerical precision)\")\n",
    "    elif max_std < 1e-3:\n",
    "        print(\"✓ GOOD: Model is highly permutation invariant\")\n",
    "    elif max_std < 0.01:\n",
    "        print(\"⚠ FAIR: Model shows some permutation variance (might need more training)\")\n",
    "    else:\n",
    "        print(\"✗ POOR: Model is NOT permutation invariant (check architecture/training)\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].hist(all_stds, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Standard Deviation', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Distribution of Prediction Std Dev Across Permutations', fontsize=14, fontweight='bold')\n",
    "    axes[0].axvline(x=0.01, color='red', linestyle='--', label='0.01 threshold', linewidth=2)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].hist(all_ranges, bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "    axes[1].set_xlabel('Range (Max - Min)', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Distribution of Prediction Range Across Permutations', fontsize=14, fontweight='bold')\n",
    "    axes[1].axvline(x=0.01, color='red', linestyle='--', label='0.01 threshold', linewidth=2)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'permutation_invariance_test.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPlot saved to {output_dir / 'permutation_invariance_test.png'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_stds, all_ranges\n",
    "\n",
    "# Test permutation invariance on test set\n",
    "stds, ranges = test_permutation_invariance(model, test_dataset, device, n_samples=100, n_permutations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17a. Test Permutation Invariance\n",
    "\n",
    "Verify that the model produces consistent predictions regardless of read order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Model Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVED TRANSFORMER MIL MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✓ Permutation-invariant Transformer MIL model successfully trained\")\n",
    "print(f\"✓ Train/Valid/Test split by gene_id (prevents data leakage)\")\n",
    "print(f\"✓ Self-attention mechanism captures read dependencies\")\n",
    "print(f\"✓ {config.NUM_LAYERS} layers, {config.NUM_HEADS} heads, {total_params:,} parameters\")\n",
    "print(f\"✓ Mixed precision training for efficiency\")\n",
    "print(f\"✓ Focal loss for class imbalance\")\n",
    "\n",
    "print(f\"\\nKEY IMPROVEMENTS:\")\n",
    "print(f\"  ✓ REMOVED positional encoding (inappropriate for unordered reads)\")\n",
    "print(f\"  ✓ Multi-head attention pooling with {config.POOLING_HEADS} learnable queries\")\n",
    "print(f\"  ✓ Permutation augmentation during training\")\n",
    "print(f\"  ✓ Truly permutation-invariant design\")\n",
    "\n",
    "print(f\"\\nKey Results:\")\n",
    "print(f\"  - Test ROC-AUC: {test_auc:.4f}\")\n",
    "print(f\"  - Test PR-AUC: {test_pr_auc:.4f}\")\n",
    "print(f\"  - Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - Model checkpoint: {checkpoint_dir / 'best_model.pt'}\")\n",
    "print(f\"  - Test predictions: {output_dir / 'test_predictions.csv'}\")\n",
    "print(f\"  - Training history: {output_dir / 'training_history.png'}\")\n",
    "print(f\"  - ROC/PR curves: {output_dir / 'test_curves.png'}\")\n",
    "print(f\"  - Multi-head attention viz: {output_dir / 'multihead_attention_examples.png'}\")\n",
    "print(f\"  - Attention heatmap: {output_dir / 'multihead_attention_heatmap.png'}\")\n",
    "print(f\"  - Summary: {output_dir / 'summary.txt'}\")\n",
    "\n",
    "print(f\"\\nWhy These Improvements Matter:\")\n",
    "print(f\"  1. NO Positional Encoding:\")\n",
    "print(f\"     - Nanopore reads have no meaningful biological order\")\n",
    "print(f\"     - Random sampling means position changes between epochs\")\n",
    "print(f\"     - Positional encoding was adding noise, not signal\")\n",
    "print(f\"  \")\n",
    "print(f\"  2. Multi-Head Attention Pooling:\")\n",
    "print(f\"     - Uses {config.POOLING_HEADS} learnable query vectors\")\n",
    "print(f\"     - Each head can focus on different read characteristics\")\n",
    "print(f\"     - More expressive than single CLS token or simple pooling\")\n",
    "print(f\"  \")\n",
    "print(f\"  3. Permutation Augmentation:\")\n",
    "print(f\"     - Randomly shuffles read order during training\")\n",
    "print(f\"     - Enforces that model doesn't learn position-dependent patterns\")\n",
    "print(f\"     - Makes model robust to any read ordering\")\n",
    "\n",
    "print(f\"\\nExpected Benefits:\")\n",
    "print(f\"  ✓ Better generalization (no spurious position patterns)\")\n",
    "print(f\"  ✓ More robust predictions (order-independent)\")\n",
    "print(f\"  ✓ Biologically appropriate (reads are naturally unordered)\")\n",
    "print(f\"  ✓ Interpretable attention (shows which reads matter, not which positions)\")\n",
    "\n",
    "print(f\"\\nNext Steps for Further Improvement:\")\n",
    "print(f\"  1. Test permutation invariance (predict on multiple shuffles, verify same result)\")\n",
    "print(f\"  2. Experiment with different numbers of pooling heads (2, 4, 8)\")\n",
    "print(f\"  3. Add learning rate scheduling (warmup + cosine decay)\")\n",
    "print(f\"  4. Try test-time augmentation (average predictions over multiple permutations)\")\n",
    "print(f\"  5. Analyze what each attention head specializes in\")\n",
    "print(f\"  6. Compare with DeepSets architecture\")\n",
    "print(f\"  7. Add read-level quality features if available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK COMPLETE - IMPROVED PERMUTATION-INVARIANT MODEL\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
