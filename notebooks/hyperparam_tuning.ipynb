{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd54408",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (lightweight, staged random search)\n",
    "\n",
    "This notebook runs a cheap hyperparameter search for the Transformer MIL model used in `transformer_2.ipynb`.\n",
    "\n",
    "Goals:\n",
    "- Run quick trials (3-5 epochs) across a randomized search space to find promising configurations\n",
    "- Re-evaluate the top-k candidates for a medium number of epochs (10-15)\n",
    "- Save best hyperparameters to `notebooks/tuning_results/best_config.json` and detailed CSV of all trials\n",
    "\n",
    "Notes:\n",
    "- This notebook duplicates only the pieces needed to run quick experiments: data loading, dataset construction, model, loss and training loop.\n",
    "- Designed to be cheap: stage-1 uses short runs to eliminate bad configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import json\n",
    "import gzip\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('..')\n",
    "DATA_FILE = ROOT / 'data' / 'dataset0.json.gz'\n",
    "LABELS_FILE = ROOT / 'data' / 'data.info.labelled'\n",
    "OUT_DIR = Path('notebooks') / 'tuning_results'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Output ->', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data loader (lightweight, matches transformer_2.ipynb format)\n",
    "\n",
    "def load_data(data_file, labels_file):\n",
    "    rows = []\n",
    "    with gzip.open(data_file, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            for transcript_id, positions in data.items():\n",
    "                for transcript_position, sequences in positions.items():\n",
    "                    for sequence, feature_list in sequences.items():\n",
    "                        for features in feature_list:\n",
    "                            rows.append({\n",
    "                                'transcript_id': transcript_id,\n",
    "                                'transcript_position': int(transcript_position),\n",
    "                                'dwell_-1': features[0],\n",
    "                                'std_-1': features[1],\n",
    "                                'mean_-1': features[2],\n",
    "                                'dwell_0': features[3],\n",
    "                                'std_0': features[4],\n",
    "                                'mean_0': features[5],\n",
    "                                'dwell_+1': features[6],\n",
    "                                'std_+1': features[7],\n",
    "                                'mean_+1': features[8],\n",
    "                            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    labels = pd.read_csv(labels_file)\n",
    "    return df, labels\n",
    "\n",
    "# Try loading a small subset check (dataset may be large; we don't print everything)\n",
    "print('Loading data... (this may take a moment)')\n",
    "df, labels = load_data(DATA_FILE, LABELS_FILE)\n",
    "print('reads:', len(df), 'labels:', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create bags and dataset (matching transformer_2's format)\n",
    "\n",
    "BASE2IDX = {'A':0, 'C':1, 'G':2, 'T':3, 'U':3}\n",
    "PAD_IDX = 4\n",
    "\n",
    "\n",
    "def seq_to_idx7(s: str):\n",
    "    s = str(s).upper().replace('U', 'T')\n",
    "    if len(s) != 7:\n",
    "        # pad or truncate defensively\n",
    "        s = (s + 'AAAAAAA')[:7]\n",
    "    return np.array([BASE2IDX.get(ch, 0) for ch in s], dtype=np.int64)\n",
    "\n",
    "num_cols = [\n",
    "    'dwell_-1','std_-1','mean_-1','dwell_0','std_0','mean_0',\n",
    "    'dwell_+1','std_+1','mean_+1'\n",
    "]\n",
    "\n",
    "site_key = ['transcript_id', 'transcript_position']\n",
    "\n",
    "\n",
    "def create_bags(df, seq_col='sequence', label_col='label', min_reads=5, max_reads=50):\n",
    "    rows = df.merge(labels, on=['transcript_id','transcript_position'], how='left')\n",
    "    grouped = rows.groupby(site_key)\n",
    "    bags = []\n",
    "    for site, group in grouped:\n",
    "        if len(group) < min_reads:\n",
    "            continue\n",
    "        feats = group[num_cols].to_numpy(dtype=np.float32)\n",
    "        seqs = group[seq_col].astype(str).tolist() if seq_col in group.columns else ['A'*7]*len(group)\n",
    "        seq_idx = np.vstack([seq_to_idx7(s) for s in seqs])\n",
    "        label = int(group[label_col].iloc[0]) if label_col in group.columns else 0\n",
    "        if len(feats) > max_reads:\n",
    "            idx = np.random.choice(len(feats), max_reads, replace=False)\n",
    "            feats = feats[idx]\n",
    "            seq_idx = seq_idx[idx]\n",
    "        bags.append({'features': feats, 'seq_idx': seq_idx, 'n_reads': len(feats), 'label': label, 'gene_id': group.get('gene_id', pd.Series()).iloc[0] if 'gene_id' in group else None})\n",
    "    print(f'Created {len(bags)} bags (min_reads={min_reads})')\n",
    "    return bags\n",
    "\n",
    "# Create base bags with conservative parameters to support tuning\n",
    "bags = create_bags(df, min_reads=5, max_reads=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used for training/validation\n",
    "class RNA_MIL_Dataset(Dataset):\n",
    "    def __init__(self, bags, bag_size=20, is_train=True, pad_idx=PAD_IDX, oversample_positive=False, oversample_factor=1):\n",
    "        self.bags = bags\n",
    "        self.bag_size = bag_size\n",
    "        self.is_train = is_train\n",
    "        if is_train and oversample_positive and oversample_factor>1:\n",
    "            pos = [b for b in bags if b['label']==1]\n",
    "            neg = [b for b in bags if b['label']==0]\n",
    "            self.bags = neg + pos * oversample_factor\n",
    "        self.proc = []\n",
    "        self.labels = []\n",
    "        for bag in self.bags:\n",
    "            num = bag['features']\n",
    "            seq = bag['seq_idx']\n",
    "            n = bag['n_reads']\n",
    "            if n == 0:\n",
    "                continue\n",
    "            if n < bag_size:\n",
    "                pad_num = np.zeros((bag_size - n, num.shape[1]), dtype=np.float32)\n",
    "                pad_seq = np.full((bag_size - n, seq.shape[1]), pad_idx, dtype=np.int64)\n",
    "                num_fixed = np.vstack([num, pad_num])\n",
    "                seq_fixed = np.vstack([seq, pad_seq])\n",
    "                mask = np.zeros(bag_size, dtype=np.float32)\n",
    "                mask[:n] = 1.0\n",
    "            else:\n",
    "                if self.is_train and n > bag_size:\n",
    "                    idx = np.random.choice(n, bag_size, replace=False)\n",
    "                else:\n",
    "                    idx = np.arange(bag_size)\n",
    "                num_fixed = num[idx].astype(np.float32)\n",
    "                seq_fixed = seq[idx].astype(np.int64)\n",
    "                mask = np.ones(bag_size, dtype=np.float32)\n",
    "            self.proc.append({'num': num_fixed, 'seq': seq_fixed, 'mask': mask})\n",
    "            self.labels.append(float(bag['label']))\n",
    "    def __len__(self):\n",
    "        return len(self.proc)\n",
    "    def __getitem__(self, idx):\n",
    "        b = self.proc[idx]\n",
    "        x_num = torch.from_numpy(b['num'])\n",
    "        x_seq = torch.from_numpy(b['seq'])\n",
    "        mask = torch.from_numpy(b['mask'])\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x_num, x_seq, mask, y\n",
    "\n",
    "print('Dataset class ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ceccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model (copied/adapted from transformer_2.ipynb)\n",
    "\n",
    "class SeqEmbCNN(nn.Module):\n",
    "    def __init__(self, vocab=5, d_emb=8, kernel_sizes=(2,3,4,5), n_filters=32, d_out=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_emb, padding_idx=PAD_IDX)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(d_emb, n_filters, ks, padding=0) for ks in kernel_sizes])\n",
    "        self.proj = nn.Linear(n_filters * len(kernel_sizes), d_out)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "    def forward(self, x_idx):\n",
    "        X = self.emb(x_idx).transpose(1,2)\n",
    "        feats = []\n",
    "        for conv in self.convs:\n",
    "            h = F.gelu(conv(X))\n",
    "            h = F.max_pool1d(h, h.shape[-1]).squeeze(-1)\n",
    "            feats.append(h)\n",
    "        z = torch.cat(feats, dim=1)\n",
    "        z = self.proj(z)\n",
    "        return self.norm(F.gelu(z))\n",
    "\n",
    "class MultiHeadAttentionPooling(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(d_model, d_model), nn.Tanh(), nn.Dropout(dropout), nn.Linear(d_model,1))\n",
    "            for _ in range(n_heads)\n",
    "        ])\n",
    "        self.fusion = nn.Sequential(nn.Linear(d_model * n_heads, d_model), nn.LayerNorm(d_model), nn.GELU())\n",
    "    def forward(self, h, mask):\n",
    "        pooled = []\n",
    "        all_weights = []\n",
    "        for attn in self.attention_heads:\n",
    "            scores = attn(h).squeeze(-1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            weights = F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "            pooled.append((h * weights).sum(dim=1))\n",
    "            all_weights.append(weights.squeeze(-1))\n",
    "        bag_repr = self.fusion(torch.cat(pooled, dim=-1))\n",
    "        avg_weights = torch.stack(all_weights, dim=0).mean(dim=0)\n",
    "        return bag_repr, avg_weights\n",
    "\n",
    "class TransformerMIL(nn.Module):\n",
    "    def __init__(self, num_features=9, d_model=256, n_heads=8, n_layers=6, d_ff=1024, dropout=0.2, attn_pool_heads=4):\n",
    "        super().__init__()\n",
    "        self.seq_encoder = SeqEmbCNN(vocab=5, d_emb=8, kernel_sizes=(2,3,4,5), n_filters=32, d_out=64)\n",
    "        self.num_proj = nn.Sequential(nn.Linear(num_features, 64), nn.LayerNorm(64), nn.GELU(), nn.Dropout(dropout))\n",
    "        self.feature_fusion = nn.Sequential(nn.Linear(128, d_model), nn.LayerNorm(d_model), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model,d_model), nn.LayerNorm(d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.attention_pool = MultiHeadAttentionPooling(d_model, n_heads=attn_pool_heads, dropout=dropout)\n",
    "        self.classifier = nn.Sequential(nn.Linear(d_model, d_model//2), nn.LayerNorm(d_model//2), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model//2, d_model//4), nn.LayerNorm(d_model//4), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model//4,1))\n",
    "        self.instance_classifier = nn.Sequential(nn.Linear(d_model, d_model//2), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model//2,1))\n",
    "    def encode_sequences(self, x_seq):\n",
    "        B, K, L = x_seq.shape\n",
    "        seq_flat = x_seq.reshape(B*K, L)\n",
    "        z_seq = self.seq_encoder(seq_flat)\n",
    "        return z_seq.view(B, K, -1)\n",
    "    def forward(self, x_num, mask, x_seq=None):\n",
    "        B, K, _ = x_num.shape\n",
    "        num_features = self.num_proj(x_num)\n",
    "        if x_seq is not None:\n",
    "            seq_features = self.encode_sequences(x_seq)\n",
    "            combined = torch.cat([num_features, seq_features], dim=-1)\n",
    "        else:\n",
    "            combined = num_features\n",
    "        h = self.feature_fusion(combined)\n",
    "        src_key_padding_mask = (mask == 0)\n",
    "        h = self.transformer(h, src_key_padding_mask=src_key_padding_mask)\n",
    "        instance_logits = self.instance_classifier(h).squeeze(-1)\n",
    "        instance_probs = torch.sigmoid(instance_logits) * mask\n",
    "        bag_repr, attention_weights = self.attention_pool(h, mask)\n",
    "        bag_logits = self.classifier(bag_repr).squeeze(-1)\n",
    "        return bag_logits, attention_weights, instance_probs\n",
    "\n",
    "print('Model definitions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb38f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss and metrics\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        alpha_weight = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        return (alpha_weight * focal_weight * bce).mean()\n",
    "\n",
    "class MILLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, focal_gamma=2.0, class_weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma=focal_gamma)\n",
    "        self.class_weight = class_weight\n",
    "    def forward(self, bag_logits, instance_probs, labels, mask):\n",
    "        bag_loss = self.focal(bag_logits, labels)\n",
    "        positive_bags = labels == 1\n",
    "        if positive_bags.any():\n",
    "            pos_instance_probs = instance_probs[positive_bags]\n",
    "            mil_pos = -torch.log(1 - torch.prod(1 - pos_instance_probs + 1e-8, dim=1) + 1e-8)\n",
    "            mil_pos = mil_pos.mean()\n",
    "        else:\n",
    "            mil_pos = torch.tensor(0.0, device=bag_logits.device)\n",
    "        return bag_loss + self.alpha * mil_pos, bag_loss, mil_pos\n",
    "\n",
    "\n",
    "def summarize_metrics(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    roc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else float('nan')\n",
    "    pr = average_precision_score(y_true, y_prob) if len(np.unique(y_true))>1 else float('nan')\n",
    "    return {'roc_auc': roc, 'pr_auc': pr}\n",
    "\n",
    "print('Loss and metric utilities ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e55846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training / evaluation loops (support scheduler)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, grad_scaler=None, mixed_precision=True, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for x_num, x_seq, mask, y in tqdm(loader, desc='Train', leave=False):\n",
    "        x_num = x_num.to(device)\n",
    "        x_seq = x_seq.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if mixed_precision and grad_scaler is not None:\n",
    "            with autocast():\n",
    "                bag_logits, _, inst_probs = model(x_num, mask, x_seq)\n",
    "                loss, bag_loss, mil_pos = criterion(bag_logits, inst_probs, y, mask)\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "        else:\n",
    "            bag_logits, _, inst_probs = model(x_num, mask, x_seq)\n",
    "            loss, bag_loss, mil_pos = criterion(bag_logits, inst_probs, y, mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            try:\n",
    "                scheduler.step()\n",
    "            except Exception:\n",
    "                pass\n",
    "        probs = torch.sigmoid(bag_logits).detach().cpu().numpy()\n",
    "        all_preds.extend(probs)\n",
    "        all_labels.extend(y.detach().cpu().numpy())\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "    metrics = summarize_metrics(all_labels, all_preds)\n",
    "    return total_loss/len(loader), metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for x_num, x_seq, mask, y in tqdm(loader, desc='Eval', leave=False):\n",
    "        x_num = x_num.to(device)\n",
    "        x_seq = x_seq.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "        bag_logits, _, inst_probs = model(x_num, mask, x_seq)\n",
    "        loss, _, _ = criterion(bag_logits, inst_probs, y, mask)\n",
    "        probs = torch.sigmoid(bag_logits).detach().cpu().numpy()\n",
    "        all_preds.extend(probs)\n",
    "        all_labels.extend(y.detach().cpu().numpy())\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "    metrics = summarize_metrics(all_labels, all_preds)\n",
    "    return total_loss/len(loader), metrics\n",
    "\n",
    "print('Training/eval loops ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f356a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Hyperparameter search: Stage 1 (quick random trials) then Stage 2 (refine)\n",
    "\n",
    "def sample_config():\n",
    "    return {\n",
    "        'lr': float(10**np.random.uniform(-5, -3)),            # 1e-5 .. 1e-3\n",
    "        'd_model': int(random.choice([128, 256])),\n",
    "        'n_heads': int(4 if random.choice([128,256])==128 else 8),\n",
    "        'n_layers': int(random.choice([4,6])),\n",
    "        'dropout': float(random.choice([0.1, 0.2, 0.3])),\n",
    "        'bag_size': int(random.choice([20, 40])),\n",
    "        'warmup_steps': int(random.choice([500,1000]))\n",
    "    }\n",
    "\n",
    "\n",
    "def run_trial(config, epochs=3, batch_size=32, device=None):\n",
    "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    # Prepare datasets\n",
    "    # split bags by simple random split: 80/10/10\n",
    "    random.shuffle(bags)\n",
    "    n = len(bags)\n",
    "    n_train = int(0.8*n)\n",
    "    n_val = int(0.1*n)\n",
    "    train_bags = bags[:n_train]\n",
    "    val_bags = bags[n_train:n_train+n_val]\n",
    "    test_bags = bags[n_train+n_val:]\n",
    "    train_ds = RNA_MIL_Dataset(train_bags, bag_size=config['bag_size'], is_train=True)\n",
    "    val_ds = RNA_MIL_Dataset(val_bags, bag_size=config['bag_size'], is_train=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    # Model\n",
    "    model = TransformerMIL(num_features=9, d_model=config['d_model'], n_heads=config['n_heads'], n_layers=config['n_layers'], dropout=config['dropout']).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1.0, (step+1)/config['warmup_steps']))\n",
    "    scaler = GradScaler()\n",
    "    criterion = MILLoss(alpha=0.3, focal_gamma=2.0)\n",
    "    best_val = -float('inf')\n",
    "    history = []\n",
    "    for ep in range(epochs):\n",
    "        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device, grad_scaler=scaler, mixed_precision=True, scheduler=scheduler)\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        val_score = (val_metrics['roc_auc'] if not math.isnan(val_metrics['roc_auc']) else 0.0) + (val_metrics['pr_auc'] if not math.isnan(val_metrics['pr_auc']) else 0.0)\n",
    "        history.append({'epoch': ep+1, 'train_loss': train_loss, 'val_loss': val_loss, **val_metrics, 'val_score': val_score})\n",
    "        if val_score > best_val:\n",
    "            best_val = val_score\n",
    "    return best_val, history\n",
    "\n",
    "# Stage 1: quick random search\n",
    "N_TRIALS = 8\n",
    "stage1_results = []\n",
    "print('Stage 1: running', N_TRIALS, 'cheap random trials (epochs=3)')\n",
    "for i in range(N_TRIALS):\n",
    "    cfg = sample_config()\n",
    "    print(f'Trial {i+1}/{N_TRIALS} cfg:', cfg)\n",
    "    best_val, hist = run_trial(cfg, epochs=3)\n",
    "    stage1_results.append({'config': cfg, 'best_val': best_val, 'history': hist})\n",
    "\n",
    "# Select top-k\n",
    "stage1_results_sorted = sorted(stage1_results, key=lambda x: x['best_val'], reverse=True)\n",
    "TOPK = min(3, len(stage1_results_sorted))\n",
    "print('Top configs from stage1:')\n",
    "for k in range(TOPK):\n",
    "    print(k+1, stage1_results_sorted[k]['config'], 'score=', stage1_results_sorted[k]['best_val'])\n",
    "\n",
    "# Stage 2: refine top configs\n",
    "stage2_results = []\n",
    "for k in range(TOPK):\n",
    "    cfg = stage1_results_sorted[k]['config']\n",
    "    print('\\nRefining config', k+1, cfg)\n",
    "    best_val, hist = run_trial(cfg, epochs=12)\n",
    "    stage2_results.append({'config': cfg, 'best_val': best_val, 'history': hist})\n",
    "\n",
    "# Choose best overall\n",
    "all_candidates = stage2_results if len(stage2_results)>0 else stage1_results_sorted\n",
    "best_overall = max(all_candidates, key=lambda x: x['best_val'])\n",
    "print('\\nBest config found:', best_overall['config'], 'score=', best_overall['best_val'])\n",
    "\n",
    "# Save results\n",
    "results_path = OUT_DIR / f'tuning_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump({'stage1': stage1_results, 'stage2': stage2_results, 'best': best_overall}, f, default=str)\n",
    "print('Saved full results to', results_path)\n",
    "\n",
    "# Save best config separately (easy to load from transformer_2 notebook)\n",
    "best_cfg_path = OUT_DIR / 'best_config.json'\n",
    "with open(best_cfg_path, 'w') as f:\n",
    "    json.dump(best_overall['config'], f, indent=2)\n",
    "print('Saved best hyperparameters to', best_cfg_path)\n",
    "\n",
    "# Also produce a CSV summary of configs\n",
    "rows = []\n",
    "for r in stage1_results:\n",
    "    rows.append({'phase':'stage1','lr':r['config']['lr'],'d_model':r['config']['d_model'],'n_layers':r['config']['n_layers'],'dropout':r['config']['dropout'],'bag_size':r['config']['bag_size'],'best_val':r['best_val']})\n",
    "for r in stage2_results:\n",
    "    rows.append({'phase':'stage2','lr':r['config']['lr'],'d_model':r['config']['d_model'],'n_layers':r['config']['n_layers'],'dropout':r['config']['dropout'],'bag_size':r['config']['bag_size'],'best_val':r['best_val']})\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_csv = OUT_DIR / 'tuning_summary.csv'\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print('Saved summary CSV to', summary_csv)\n",
    "\n",
    "print('\\nTuning complete. Next steps:')\n",
    "print('- Open', best_cfg_path, 'and copy values into your training notebook')\n",
    "print('- Or modify `transformer_2.ipynb` to load this JSON at the configuration section')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2a9b3",
   "metadata": {},
   "source": [
    "## How to use the best hyperparameters\n",
    "\n",
    "In `transformer_2.ipynb`:\n",
    "- At the top where the training config is created, you can add:\n",
    "\n",
    "```python\n",
    "import json\n",
    "best = json.load(open('notebooks/tuning_results/best_config.json'))\n",
    "config.LEARNING_RATE = best['lr']\n",
    "config.EMBED_DIM = best['d_model']  # or d_model mapping if different naming\n",
    "config.NUM_LAYERS = best['n_layers']\n",
    "config.DROPOUT = best['dropout']\n",
    "config.N_READS_PER_SITE = best['bag_size']\n",
    "config.WARMUP_STEPS = best['warmup_steps']\n",
    "```\n",
    "\n",
    "Or manually copy values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eede13",
   "metadata": {},
   "source": [
    "## 8. Optuna Hyperparameter Tuning\n",
    "\n",
    "We use Optuna with TPE sampler and MedianPruner. The objective maximizes ROC-AUC + PR-AUC on the validation split. Trials are pruned early if progress is poor.\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Optuna study configuration\n",
    "N_TRIALS = 20           # adjust down/up based on your budget\n",
    "MAX_EPOCHS = 12         # per trial (uses pruning, so many trials stop early)\n",
    "STARTUP_TRIALS = 4      # number of trials before pruning kicks in\n",
    "PRUNER_WARMUP_EPOCHS = 2\n",
    "\n",
    "study_dir = OUT_DIR\n",
    "study_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    d_model = trial.suggest_categorical('d_model', [128, 256])\n",
    "    n_layers = trial.suggest_categorical('n_layers', [4, 6])\n",
    "    dropout = trial.suggest_categorical('dropout', [0.1, 0.2, 0.3])\n",
    "    bag_size = trial.suggest_categorical('bag_size', [20, 40])\n",
    "    warmup_steps = trial.suggest_categorical('warmup_steps', [500, 1000])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Random 80/10/10 split per trial to reduce leakage between configs\n",
    "    local_bags = bags.copy()\n",
    "    random.shuffle(local_bags)\n",
    "    n = len(local_bags)\n",
    "    n_train = int(0.8*n)\n",
    "    n_val = int(0.1*n)\n",
    "    train_bags = local_bags[:n_train]\n",
    "    val_bags = local_bags[n_train:n_train+n_val]\n",
    "\n",
    "    train_ds = RNA_MIL_Dataset(train_bags, bag_size=bag_size, is_train=True)\n",
    "    val_ds = RNA_MIL_Dataset(val_bags, bag_size=bag_size, is_train=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Model and training setup\n",
    "    model = TransformerMIL(num_features=9, d_model=d_model, n_heads=(4 if d_model==128 else 8), n_layers=n_layers, dropout=dropout).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1.0, (step+1)/warmup_steps))\n",
    "    scaler = GradScaler()\n",
    "    criterion = MILLoss(alpha=0.3, focal_gamma=2.0)\n",
    "\n",
    "    best_score = -float('inf')\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS+1):\n",
    "        # one epoch train\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device, grad_scaler=scaler, mixed_precision=True, scheduler=scheduler)\n",
    "        # evaluate\n",
    "        _, val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        val_auc = 0.0 if (math.isnan(val_metrics['roc_auc'])) else val_metrics['roc_auc']\n",
    "        val_pr = 0.0 if (math.isnan(val_metrics['pr_auc'])) else val_metrics['pr_auc']\n",
    "        score = val_auc + val_pr\n",
    "\n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(score, step=epoch)\n",
    "        if trial.should_prune() and epoch >= PRUNER_WARMUP_EPOCHS:\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "    return best_score\n",
    "\n",
    "# Create study with TPE sampler + median pruner\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_startup_trials=STARTUP_TRIALS, n_warmup_steps=PRUNER_WARMUP_EPOCHS)\n",
    ")\n",
    "\n",
    "print('Starting Optuna study...')\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best value:', study.best_value)\n",
    "print('Best params:', study.best_params)\n",
    "\n",
    "# Save best config\n",
    "best_cfg = study.best_params.copy()\n",
    "# Ensure required keys for downstream notebook\n",
    "if 'n_heads' not in best_cfg:\n",
    "    best_cfg['n_heads'] = 4 if best_cfg['d_model'] == 128 else 8\n",
    "best_cfg_path = study_dir / 'best_config_optuna.json'\n",
    "with open(best_cfg_path, 'w') as f:\n",
    "    json.dump(best_cfg, f, indent=2)\n",
    "print('Saved best config to', best_cfg_path)\n",
    "\n",
    "# Save trials dataframe\n",
    "try:\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_csv = study_dir / 'optuna_trials.csv'\n",
    "    trials_df.to_csv(trials_csv, index=False)\n",
    "    print('Saved trials to', trials_csv)\n",
    "except Exception as e:\n",
    "    print('Could not save trials dataframe:', e)\n",
    "\n",
    "print('Best (Optuna):', study.best_value, '\\nParams:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Optuna study configuration\n",
    "N_TRIALS = 20           # adjust down/up based on your budget\n",
    "MAX_EPOCHS = 12         # per trial (uses pruning, so many trials stop early)\n",
    "STARTUP_TRIALS = 4      # number of trials before pruning kicks in\n",
    "PRUNER_WARMUP_EPOCHS = 2\n",
    "\n",
    "study_dir = OUT_DIR\n",
    "study_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    d_model = trial.suggest_categorical('d_model', [128, 256])\n",
    "    n_layers = trial.suggest_categorical('n_layers', [4, 6])\n",
    "    dropout = trial.suggest_categorical('dropout', [0.1, 0.2, 0.3])\n",
    "    bag_size = trial.suggest_categorical('bag_size', [20, 40])\n",
    "    warmup_steps = trial.suggest_categorical('warmup_steps', [500, 1000])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Random 80/10/10 split per trial to reduce leakage between configs\n",
    "    local_bags = bags.copy()\n",
    "    random.shuffle(local_bags)\n",
    "    n = len(local_bags)\n",
    "    n_train = int(0.8*n)\n",
    "    n_val = int(0.1*n)\n",
    "    train_bags = local_bags[:n_train]\n",
    "    val_bags = local_bags[n_train:n_train+n_val]\n",
    "\n",
    "    train_ds = RNA_MIL_Dataset(train_bags, bag_size=bag_size, is_train=True)\n",
    "    val_ds = RNA_MIL_Dataset(val_bags, bag_size=bag_size, is_train=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Model and training setup\n",
    "    model = TransformerMIL(num_features=9, d_model=d_model, n_heads=(4 if d_model==128 else 8), n_layers=n_layers, dropout=dropout).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1.0, (step+1)/warmup_steps))\n",
    "    scaler = GradScaler()\n",
    "    criterion = MILLoss(alpha=0.3, focal_gamma=2.0)\n",
    "\n",
    "    best_score = -float('inf')\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS+1):\n",
    "        # Train one epoch (quietly - no progress bar)\n",
    "        model.train()\n",
    "        for x_num, x_seq, mask, y in train_loader:\n",
    "            x_num, x_seq, mask, y = x_num.to(device), x_seq.to(device), mask.to(device), y.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                bag_logits, _, inst_probs = model(x_num, mask, x_seq)\n",
    "                loss, _, _ = criterion(bag_logits, inst_probs, y, mask)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Evaluate (quietly)\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_num, x_seq, mask, y in val_loader:\n",
    "                x_num, x_seq, mask, y = x_num.to(device), x_seq.to(device), mask.to(device), y.to(device)\n",
    "                bag_logits, _, _ = model(x_num, mask, x_seq)\n",
    "                probs = torch.sigmoid(bag_logits).cpu().numpy()\n",
    "                all_preds.extend(probs)\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "        \n",
    "        val_metrics = summarize_metrics(all_labels, all_preds)\n",
    "        val_auc = 0.0 if math.isnan(val_metrics['roc_auc']) else val_metrics['roc_auc']\n",
    "        val_pr = 0.0 if math.isnan(val_metrics['pr_auc']) else val_metrics['pr_auc']\n",
    "        score = val_auc + val_pr\n",
    "\n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(score, step=epoch)\n",
    "        if trial.should_prune() and epoch >= PRUNER_WARMUP_EPOCHS:\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "    return best_score\n",
    "\n",
    "# Create study with TPE sampler + median pruner\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_startup_trials=STARTUP_TRIALS, n_warmup_steps=PRUNER_WARMUP_EPOCHS)\n",
    ")\n",
    "\n",
    "print('Starting Optuna study...')\n",
    "# Disable progress bar to reduce output spam, use verbose logging instead\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best value:', study.best_value)\n",
    "print('Best params:', study.best_params)\n",
    "\n",
    "# Save best config\n",
    "best_cfg = study.best_params.copy()\n",
    "# Ensure required keys for downstream notebook\n",
    "if 'n_heads' not in best_cfg:\n",
    "    best_cfg['n_heads'] = 4 if best_cfg['d_model'] == 128 else 8\n",
    "best_cfg_path = study_dir / 'best_config_optuna.json'\n",
    "with open(best_cfg_path, 'w') as f:\n",
    "    json.dump(best_cfg, f, indent=2)\n",
    "print('Saved best config to', best_cfg_path)\n",
    "\n",
    "# Save trials dataframe\n",
    "try:\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_csv = study_dir / 'optuna_trials.csv'\n",
    "    trials_df.to_csv(trials_csv, index=False)\n",
    "    print('Saved trials to', trials_csv)\n",
    "except Exception as e:\n",
    "    print('Could not save trials dataframe:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best (Optuna) score:', study.best_value)\n",
    "print('Best (Optuna) params:', study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
