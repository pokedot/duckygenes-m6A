{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4e164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb3126",
   "metadata": {},
   "source": [
    "# 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77019687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading trained CNN model...\n",
      "✅ Successfully loaded model from: ../models/final_cnn_model.keras\n",
      "\n",
      "📊 Model Architecture:\n",
      "  Input shape: (None, 3, 57)\n",
      "  Output shape: (None, 1)\n",
      "  Total parameters: 69,089\n",
      "✅ Successfully loaded model from: ../models/final_cnn_model.keras\n",
      "\n",
      "📊 Model Architecture:\n",
      "  Input shape: (None, 3, 57)\n",
      "  Output shape: (None, 1)\n",
      "  Total parameters: 69,089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inbam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 26 variables whereas the saved optimizer has 50 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m22,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m24,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m4,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">137,348</span> (536.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m137,348\u001b[0m (536.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,257</span> (266.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m68,257\u001b[0m (266.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> (3.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m832\u001b[0m (3.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,259</span> (266.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m68,259\u001b[0m (266.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained CNN model\n",
    "model_path = \"../models/final_cnn_model.keras\"\n",
    "legacy_model_path = \"../models/final_cnn_model_legacy.h5\"\n",
    "\n",
    "print(\"🔄 Loading trained CNN model...\")\n",
    "\n",
    "try:\n",
    "    # Try to load modern Keras format first\n",
    "    if os.path.exists(model_path):\n",
    "        model = keras.models.load_model(model_path)\n",
    "        print(f\"✅ Successfully loaded model from: {model_path}\")\n",
    "    elif os.path.exists(legacy_model_path):\n",
    "        model = keras.models.load_model(legacy_model_path)\n",
    "        print(f\"✅ Successfully loaded legacy model from: {legacy_model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No trained model found!\")\n",
    "        \n",
    "    # Display model architecture\n",
    "    print(f\"\\n📊 Model Architecture:\")\n",
    "    print(f\"  Input shape: {model.input_shape}\")\n",
    "    print(f\"  Output shape: {model.output_shape}\")\n",
    "    print(f\"  Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Show model summary\n",
    "    model.summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Make sure you have trained the model first using cnn.ipynb\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221501f",
   "metadata": {},
   "source": [
    "# 2. Define Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4287e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction functions (same as training)\n",
    "def flatten_deep_data(file_path, estimated_rows=11000000):\n",
    "    \"\"\"\n",
    "    Ultra-optimized data loading with batch processing\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 10000\n",
    "    GROWTH_FACTOR = 1.5\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    capacity = estimated_rows\n",
    "    transcript_ids = np.empty(capacity, dtype=object)\n",
    "    positions = np.empty(capacity, dtype=np.int32)\n",
    "    seq = np.empty(capacity, dtype=object)\n",
    "    feature_arrays = [np.empty(capacity, dtype=np.float32) for _ in range(9)]\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    # Temporary batch storage\n",
    "    batch_transcript_ids = []\n",
    "    batch_positions = []\n",
    "    batch_seq = []\n",
    "    batch_features = [[] for _ in range(9)]\n",
    "    \n",
    "    def flush_batch():\n",
    "        \"\"\"Flush batch to main arrays\"\"\"\n",
    "        nonlocal idx, capacity\n",
    "        \n",
    "        batch_size = len(batch_transcript_ids)\n",
    "        if batch_size == 0:\n",
    "            return\n",
    "        \n",
    "        # Resize if needed\n",
    "        while idx + batch_size > capacity:\n",
    "            new_capacity = int(capacity * GROWTH_FACTOR)\n",
    "            transcript_ids.resize(new_capacity, refcheck=False)\n",
    "            positions.resize(new_capacity, refcheck=False)\n",
    "            seq.resize(new_capacity, refcheck=False)\n",
    "            for i in range(9):\n",
    "                feature_arrays[i].resize(new_capacity, refcheck=False)\n",
    "            capacity = new_capacity\n",
    "        \n",
    "        # Bulk assignment\n",
    "        transcript_ids[idx:idx+batch_size] = batch_transcript_ids\n",
    "        positions[idx:idx+batch_size] = batch_positions\n",
    "        seq[idx:idx+batch_size] = batch_seq\n",
    "        for i in range(9):\n",
    "            feature_arrays[i][idx:idx+batch_size] = batch_features[i]\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "        # Clear batch\n",
    "        batch_transcript_ids.clear()\n",
    "        batch_positions.clear()\n",
    "        batch_seq.clear()\n",
    "        for lst in batch_features:\n",
    "            lst.clear()\n",
    "    \n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            for transcript_id, positions_dict in data.items():\n",
    "                for transcript_position, sequences in positions_dict.items():\n",
    "                    pos_int = int(transcript_position)\n",
    "                    for sequence, feature_list in sequences.items():\n",
    "                        for features in feature_list:\n",
    "                            # Add to batch\n",
    "                            batch_transcript_ids.append(transcript_id)\n",
    "                            batch_positions.append(pos_int)\n",
    "                            batch_seq.append(sequence)\n",
    "                            for i, val in enumerate(features):\n",
    "                                batch_features[i].append(val)\n",
    "                            \n",
    "                            # Flush when batch is full\n",
    "                            if len(batch_transcript_ids) >= BATCH_SIZE:\n",
    "                                flush_batch()\n",
    "                                \n",
    "                                if idx % 100000 == 0:\n",
    "                                    print(f\"Processed {idx:,} rows...\", end='\\r')\n",
    "    \n",
    "    # Flush remaining batch\n",
    "    flush_batch()\n",
    "    \n",
    "    print(f\"\\nCreating DataFrame with {idx:,} rows...\")\n",
    "    \n",
    "    # Trim and create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'transcript_id': transcript_ids[:idx],\n",
    "        'transcript_position': positions[:idx],\n",
    "        'sequence': seq[:idx],\n",
    "        'dwell_-1': feature_arrays[0][:idx],\n",
    "        'std_-1': feature_arrays[1][:idx],\n",
    "        'mean_-1': feature_arrays[2][:idx],\n",
    "        'dwell_0': feature_arrays[3][:idx],\n",
    "        'std_0': feature_arrays[4][:idx],\n",
    "        'mean_0': feature_arrays[5][:idx],\n",
    "        'dwell_+1': feature_arrays[6][:idx],\n",
    "        'std_+1': feature_arrays[7][:idx],\n",
    "        'mean_+1': feature_arrays[8][:idx],\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_cnn_features(group):\n",
    "    \"\"\"Create CNN features with sliding window 5-mer approach (same as training)\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Define positions and their feature types\n",
    "    positions = ['-1', '0', '+1']\n",
    "    feature_types = ['dwell', 'std', 'mean']\n",
    "    \n",
    "    # 1. ORIGINAL SIGNAL FEATURES (preserving spatial organization)\n",
    "    for pos in positions:\n",
    "        for feat_type in feature_types:\n",
    "            col = f'{feat_type}_{pos}'\n",
    "            # Create all 9 statistics for this feature at this position\n",
    "            features[f'{col}_mean'] = group[col].mean()\n",
    "            features[f'{col}_median'] = group[col].median()\n",
    "            features[f'{col}_std'] = group[col].std()\n",
    "            features[f'{col}_iqr'] = group[col].quantile(0.75) - group[col].quantile(0.25)\n",
    "            features[f'{col}_skew'] = group[col].skew()\n",
    "            features[f'{col}_min'] = group[col].min()\n",
    "            features[f'{col}_max'] = group[col].max()\n",
    "            features[f'{col}_q25'] = group[col].quantile(0.25)\n",
    "            features[f'{col}_q75'] = group[col].quantile(0.75)\n",
    "    \n",
    "    # 2. SEQUENCE ONE-HOT ENCODING (sliding window 5-mers)\n",
    "    consensus_sequence = group['sequence'].iloc[0]  # 7-mer sequence\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    \n",
    "    # Define 5-mer windows for each CNN position\n",
    "    cnn_position_windows = {\n",
    "        '-1': [0, 1, 2, 3, 4],  # Characters 1-5 (indices 0-4)\n",
    "        '0':  [1, 2, 3, 4, 5],  # Characters 2-6 (indices 1-5)\n",
    "        '+1': [2, 3, 4, 5, 6]   # Characters 3-7 (indices 2-6)\n",
    "    }\n",
    "    \n",
    "    # For each CNN position, create one-hot features for the 5-mer window\n",
    "    for pos in positions:\n",
    "        seq_indices = cnn_position_windows[pos]\n",
    "        \n",
    "        for seq_idx in seq_indices:\n",
    "            if seq_idx < len(consensus_sequence):\n",
    "                nucleotide = consensus_sequence[seq_idx]\n",
    "                seq_pos_label = seq_idx - 3  # Convert to relative position (-3 to +3)\n",
    "                \n",
    "                # Create one-hot encoding for this sequence position\n",
    "                for nt in nucleotides:\n",
    "                    features[f'seq_pos{seq_pos_label}_{nt}_{pos}'] = 1 if nucleotide == nt else 0\n",
    "    \n",
    "    # 3. ADDITIONAL SEQUENCE FEATURES (5-mer composition)\n",
    "    for pos in positions:\n",
    "        seq_indices = cnn_position_windows[pos]\n",
    "        \n",
    "        # Count nucleotides for this CNN position's 5-mer window\n",
    "        pos_sequence = ''.join([consensus_sequence[i] for i in seq_indices if i < len(consensus_sequence)])\n",
    "        \n",
    "        for nt in nucleotides:\n",
    "            features[f'nt_count_{nt}_{pos}'] = pos_sequence.count(nt)\n",
    "            features[f'nt_freq_{nt}_{pos}'] = pos_sequence.count(nt) / len(pos_sequence) if pos_sequence else 0\n",
    "        \n",
    "        # Purine/Pyrimidine for this position's 5-mer\n",
    "        purines = sum(1 for n in pos_sequence if n in ['A', 'G'])\n",
    "        features[f'purine_count_{pos}'] = purines\n",
    "        features[f'purine_freq_{pos}'] = purines / len(pos_sequence) if pos_sequence else 0\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "print(\"✅ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b201b6",
   "metadata": {},
   "source": [
    "# 3. Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5117a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data preparation functions defined\n"
     ]
    }
   ],
   "source": [
    "def prepare_inference_data(features_df, scaler=None):\n",
    "    \"\"\"Prepare data for CNN inference (no labels needed)\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Preparing inference data...\")\n",
    "    print(f\"Total features available: {features_df.shape[1]}\")\n",
    "    \n",
    "    # Organize features by position for proper CNN spatial structure\n",
    "    feature_names = features_df.columns\n",
    "    \n",
    "    # Group ALL features by position (-1, 0, +1)\n",
    "    pos_minus1_features = [f for f in feature_names if '_-1' in f]\n",
    "    pos_0_features = [f for f in feature_names if '_0' in f and '_-1' not in f and '_+1' not in f]  \n",
    "    pos_plus1_features = [f for f in feature_names if '_+1' in f]\n",
    "    \n",
    "    print(f\"Features per position: -1={len(pos_minus1_features)}, 0={len(pos_0_features)}, +1={len(pos_plus1_features)}\")\n",
    "    \n",
    "    # Check that features are evenly distributed across positions\n",
    "    if len(pos_minus1_features) != len(pos_0_features) or len(pos_0_features) != len(pos_plus1_features):\n",
    "        print(f\"⚠️  Warning: Uneven feature distribution across positions!\")\n",
    "        print(f\"Position -1 features: {len(pos_minus1_features)}\")\n",
    "        print(f\"Position 0 features: {len(pos_0_features)}\")  \n",
    "        print(f\"Position +1 features: {len(pos_plus1_features)}\")\n",
    "    \n",
    "    features_per_position = len(pos_minus1_features)\n",
    "    \n",
    "    # Create properly ordered feature matrix\n",
    "    ordered_features = pos_minus1_features + pos_0_features + pos_plus1_features\n",
    "    feature_matrix = features_df[ordered_features].values\n",
    "    \n",
    "    # Reshape to (N_samples, 3_positions, features_per_position)\n",
    "    feature_matrix = feature_matrix.reshape(-1, 3, features_per_position)\n",
    "    print(f\"Reshaped feature matrix: {feature_matrix.shape} (samples, positions, features_per_position)\")\n",
    "    \n",
    "    # Scale features if scaler provided\n",
    "    if scaler is not None:\n",
    "        # Reshape to 2D, scale, reshape back\n",
    "        original_shape = feature_matrix.shape\n",
    "        feature_matrix_scaled = scaler.transform(feature_matrix.reshape(-1, features_per_position)).reshape(original_shape)\n",
    "        print(f\"✅ Features scaled using provided scaler\")\n",
    "        return feature_matrix_scaled\n",
    "    else:\n",
    "        print(\"⚠️  No scaler provided - using raw features (may affect performance)\")\n",
    "        return feature_matrix\n",
    "\n",
    "def create_submission_file(transcript_ids, positions, predictions, output_path):\n",
    "    \"\"\"Create submission file with predictions\"\"\"\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'transcript_id': transcript_ids,\n",
    "        'transcript_position': positions,\n",
    "        'score': predictions.flatten()\n",
    "    })\n",
    "    \n",
    "    # Create predictions directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Submission file saved: {output_path}\")\n",
    "    print(f\"   Shape: {submission_df.shape}\")\n",
    "    print(f\"   Sample predictions:\")\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📊 Prediction Statistics:\")\n",
    "    print(f\"   Mean score: {submission_df['score'].mean():.4f}\")\n",
    "    print(f\"   Std score: {submission_df['score'].std():.4f}\")\n",
    "    print(f\"   Min score: {submission_df['score'].min():.4f}\")\n",
    "    print(f\"   Max score: {submission_df['score'].max():.4f}\")\n",
    "    print(f\"   Predicted positives (>0.5): {(submission_df['score'] > 0.5).sum()}\")\n",
    "    print(f\"   Predicted positives (%): {(submission_df['score'] > 0.5).mean() * 100:.2f}%\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "print(\"✅ Data preparation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98603fe6",
   "metadata": {},
   "source": [
    "# 4. Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387e8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "def run_inference_pipeline(dataset_name, dataset_file, scaler=None):\n",
    "    \"\"\"\n",
    "    Complete inference pipeline for a dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name for output file (e.g., 'dataset0', 'dataset1')\n",
    "        dataset_file: Path to dataset JSON.gz file\n",
    "        scaler: Fitted scaler from training (optional but recommended)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Starting inference pipeline for {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load and process data\n",
    "    print(f\"📁 Loading data from {dataset_file}...\")\n",
    "    df = flatten_deep_data(dataset_file)\n",
    "    \n",
    "    print(f\"📊 Dataset statistics:\")\n",
    "    print(f\"   Total rows: {len(df):,}\")\n",
    "    print(f\"   Unique positions: {df.groupby(['transcript_id', 'transcript_position']).ngroups:,}\")\n",
    "    \n",
    "    # 2. Extract features\n",
    "    print(f\"\\n🔧 Extracting CNN features...\")\n",
    "    tqdm.pandas()\n",
    "    cnn_features = df.groupby(['transcript_id', 'transcript_position']).progress_apply(\n",
    "        create_cnn_features, include_groups=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Feature extraction complete:\")\n",
    "    print(f\"   Feature matrix shape: {cnn_features.shape}\")\n",
    "    print(f\"   Features per position: {cnn_features.shape[1] // 3}\")\n",
    "    \n",
    "    # 3. Prepare data for inference\n",
    "    X_inference = prepare_inference_data(cnn_features, scaler=scaler)\n",
    "    \n",
    "    # 4. Generate predictions\n",
    "    print(f\"\\n🤖 Generating predictions...\")\n",
    "    predictions = model.predict(X_inference, batch_size=256, verbose=1)\n",
    "    \n",
    "    # 5. Prepare output data\n",
    "    transcript_ids = [idx[0] for idx in cnn_features.index]\n",
    "    positions = [idx[1] for idx in cnn_features.index]\n",
    "    \n",
    "    # 6. Create submission file\n",
    "    output_path = f\"../predictions/{dataset_name}_predictions_cnn.csv\"\n",
    "    submission_df = create_submission_file(transcript_ids, positions, predictions, output_path)\n",
    "    \n",
    "    print(f\"\\n✅ Inference pipeline completed for {dataset_name}\")\n",
    "    return submission_df\n",
    "\n",
    "print(\"✅ Inference pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3517d",
   "metadata": {},
   "source": [
    "# 5. Run Inference on Available Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8d93b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 3 datasets:\n",
      "   - dataset0: ../data/dataset0.json.gz\n",
      "   - dataset1: ../data/dataset1.json.gz\n",
      "   - dataset2: ../data/dataset2.json.gz\n",
      "\n",
      "✅ Ready to run inference on 3 datasets\n"
     ]
    }
   ],
   "source": [
    "# Discover available datasets\n",
    "data_dir = \"../data/\"\n",
    "available_datasets = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.startswith(\"dataset\") and file.endswith(\".json.gz\"):\n",
    "        dataset_name = file.replace(\".json.gz\", \"\")\n",
    "        dataset_path = os.path.join(data_dir, file)\n",
    "        available_datasets.append((dataset_name, dataset_path))\n",
    "\n",
    "print(f\"🔍 Found {len(available_datasets)} datasets:\")\n",
    "for name, path in available_datasets:\n",
    "    print(f\"   - {name}: {path}\")\n",
    "\n",
    "if not available_datasets:\n",
    "    print(\"⚠️  No datasets found in ../data/ directory\")\n",
    "    print(\"   Make sure dataset files are named like: dataset0.json.gz, dataset1.json.gz, etc.\")\n",
    "else:\n",
    "    print(f\"\\n✅ Ready to run inference on {len(available_datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802d4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded fitted scaler from: ../models/cnn_scaler.pkl\n",
      "\n",
      "================================================================================\n",
      "🚀 Processing dataset0\n",
      "================================================================================\n",
      "\n",
      "🚀 Starting inference pipeline for dataset0\n",
      "============================================================\n",
      "📁 Loading data from ../data/dataset0.json.gz...\n",
      "Processed 11,000,000 rows...\n",
      "Creating DataFrame with 11,027,106 rows...\n",
      "\n",
      "Creating DataFrame with 11,027,106 rows...\n",
      "📊 Dataset statistics:\n",
      "   Total rows: 11,027,106\n",
      "📊 Dataset statistics:\n",
      "   Total rows: 11,027,106\n",
      "   Unique positions: 121,838\n",
      "\n",
      "🔧 Extracting CNN features...\n",
      "   Unique positions: 121,838\n",
      "\n",
      "🔧 Extracting CNN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11423/121838 [03:10<30:40, 59.99it/s] \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run inference pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m submission_df = \u001b[43mrun_inference_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m submission_files[dataset_name] = submission_df\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Successfully processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mrun_inference_pipeline\u001b[39m\u001b[34m(dataset_name, dataset_file, scaler)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔧 Extracting CNN features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m tqdm.pandas()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m cnn_features = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtranscript_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtranscript_position\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_cnn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_groups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Feature extraction complete:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Feature matrix shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnn_features.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\std.py:917\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[39m\u001b[34m(df, func, *args, **kwargs)\u001b[39m\n\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    919\u001b[39m     t.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\groupby\\groupby.py:1820\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1817\u001b[39m     f = func\n\u001b[32m   1819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_groups:\n\u001b[32m-> \u001b[39m\u001b[32m1820\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj_with_exclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m   1823\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\groupby\\groupby.py:1886\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1853\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1858\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1859\u001b[39m ) -> NDFrameT:\n\u001b[32m   1860\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1861\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1862\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1884\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1885\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1886\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1887\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1888\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\std.py:912\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    907\u001b[39m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[32m    908\u001b[39m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[32m    910\u001b[39m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[32m    911\u001b[39m     t.update(n=\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.total \u001b[38;5;129;01mor\u001b[39;00m t.n < t.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mcreate_cnn_features\u001b[39m\u001b[34m(group)\u001b[39m\n\u001b[32m    117\u001b[39m features[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_median\u001b[39m\u001b[33m'\u001b[39m] = group[col].median()\n\u001b[32m    118\u001b[39m features[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_std\u001b[39m\u001b[33m'\u001b[39m] = group[col].std()\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m features[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_iqr\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m - group[col].quantile(\u001b[32m0.25\u001b[39m)\n\u001b[32m    120\u001b[39m features[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_skew\u001b[39m\u001b[33m'\u001b[39m] = group[col].skew()\n\u001b[32m    121\u001b[39m features[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_min\u001b[39m\u001b[33m'\u001b[39m] = group[col].min()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\series.py:2898\u001b[39m, in \u001b[36mSeries.quantile\u001b[39m\u001b[34m(self, q, interpolation)\u001b[39m\n\u001b[32m   2894\u001b[39m \u001b[38;5;66;03m# We dispatch to DataFrame so that core.internals only has to worry\u001b[39;00m\n\u001b[32m   2895\u001b[39m \u001b[38;5;66;03m#  about 2D cases.\u001b[39;00m\n\u001b[32m   2896\u001b[39m df = \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m-> \u001b[39m\u001b[32m2898\u001b[39m result = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2899\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   2900\u001b[39m     result = result.iloc[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:12153\u001b[39m, in \u001b[36mDataFrame.quantile\u001b[39m\u001b[34m(self, q, axis, numeric_only, interpolation, method)\u001b[39m\n\u001b[32m  12147\u001b[39m axis = \u001b[38;5;28mself\u001b[39m._get_axis_number(axis)\n\u001b[32m  12149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(q):\n\u001b[32m  12150\u001b[39m     \u001b[38;5;66;03m# BlockManager.quantile expects listlike, so we wrap and unwrap here\u001b[39;00m\n\u001b[32m  12151\u001b[39m     \u001b[38;5;66;03m# error: List item 0 has incompatible type \"float | ExtensionArray |\u001b[39;00m\n\u001b[32m  12152\u001b[39m     \u001b[38;5;66;03m# ndarray[Any, Any] | Index | Series | Sequence[float]\"; expected \"float\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m12153\u001b[39m     res_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12154\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[list-item]\u001b[39;49;00m\n\u001b[32m  12155\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12157\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12159\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  12160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  12161\u001b[39m         res = res_df.iloc[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:12198\u001b[39m, in \u001b[36mDataFrame.quantile\u001b[39m\u001b[34m(self, q, axis, numeric_only, interpolation, method)\u001b[39m\n\u001b[32m  12194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m  12195\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Method must be in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m  12196\u001b[39m     )\n\u001b[32m  12197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12198\u001b[39m     res = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  12199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  12200\u001b[39m     valid_interpolation = {\u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlower\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhigher\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\internals\\managers.py:1549\u001b[39m, in \u001b[36mBlockManager.quantile\u001b[39m\u001b[34m(self, qs, interpolation)\u001b[39m\n\u001b[32m   1545\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m   1546\u001b[39m new_axes[\u001b[32m1\u001b[39m] = Index(qs, dtype=np.float64)\n\u001b[32m   1548\u001b[39m blocks = [\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m     \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m   1550\u001b[39m ]\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(blocks, new_axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\internals\\blocks.py:1957\u001b[39m, in \u001b[36mBlock.quantile\u001b[39m\u001b[34m(self, qs, interpolation)\u001b[39m\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m   1955\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m is_list_like(qs)  \u001b[38;5;66;03m# caller is responsible for this\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1957\u001b[39m result = \u001b[43mquantile_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqs\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[38;5;66;03m# ensure_block_shape needed for cases where we start with EA and result\u001b[39;00m\n\u001b[32m   1959\u001b[39m \u001b[38;5;66;03m#  is ndarray, e.g. IntegerArray, SparseArray\u001b[39;00m\n\u001b[32m   1960\u001b[39m result = ensure_block_shape(result, ndim=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\array_algos\\quantile.py:39\u001b[39m, in \u001b[36mquantile_compat\u001b[39m\u001b[34m(values, qs, interpolation)\u001b[39m\n\u001b[32m     37\u001b[39m     fill_value = na_value_for_dtype(values.dtype, compat=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m     mask = isna(values)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquantile_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m values._quantile(qs, interpolation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\array_algos\\quantile.py:97\u001b[39m, in \u001b[36mquantile_with_mask\u001b[39m\u001b[34m(values, mask, fill_value, qs, interpolation)\u001b[39m\n\u001b[32m     95\u001b[39m     result = np.repeat(flat, \u001b[38;5;28mlen\u001b[39m(values)).reshape(\u001b[38;5;28mlen\u001b[39m(values), \u001b[38;5;28mlen\u001b[39m(qs))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     result = \u001b[43m_nanpercentile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqs\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     result = np.asarray(result)\n\u001b[32m    106\u001b[39m     result = result.T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\array_algos\\quantile.py:218\u001b[39m, in \u001b[36m_nanpercentile\u001b[39m\u001b[34m(values, qs, na_value, mask, interpolation)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: No overload variant of \"percentile\" matches argument types\u001b[39;49;00m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"ndarray[Any, Any]\", \"ndarray[Any, dtype[floating[_64Bit]]]\",\u001b[39;49;00m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"int\", \"Dict[str, str]\"  [call-overload]\u001b[39;49;00m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:4290\u001b[39m, in \u001b[36mpercentile\u001b[39m\u001b[34m(a, q, axis, out, overwrite_input, method, keepdims, weights, interpolation)\u001b[39m\n\u001b[32m   4287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.any(weights < \u001b[32m0\u001b[39m):\n\u001b[32m   4288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWeights must be non-negative.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4291\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:4567\u001b[39m, in \u001b[36m_quantile_unchecked\u001b[39m\u001b[34m(a, q, axis, out, overwrite_input, method, keepdims, weights)\u001b[39m\n\u001b[32m   4558\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_quantile_unchecked\u001b[39m(a,\n\u001b[32m   4559\u001b[39m                         q,\n\u001b[32m   4560\u001b[39m                         axis=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4564\u001b[39m                         keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   4565\u001b[39m                         weights=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   4566\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4568\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4569\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4570\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4571\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4572\u001b[39m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4573\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4574\u001b[39m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4575\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:3912\u001b[39m, in \u001b[36m_ureduce\u001b[39m\u001b[34m(a, func, keepdims, **kwargs)\u001b[39m\n\u001b[32m   3909\u001b[39m     index_out = (\u001b[32m0\u001b[39m, ) * nd\n\u001b[32m   3910\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out[(\u001b[38;5;28mEllipsis\u001b[39m, ) + index_out]\n\u001b[32m-> \u001b[39m\u001b[32m3912\u001b[39m r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3914\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3915\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:4742\u001b[39m, in \u001b[36m_quantile_ureduce_func\u001b[39m\u001b[34m(a, q, weights, axis, out, overwrite_input, method)\u001b[39m\n\u001b[32m   4740\u001b[39m     arr = a.copy()\n\u001b[32m   4741\u001b[39m     wgt = weights\n\u001b[32m-> \u001b[39m\u001b[32m4742\u001b[39m result = \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4743\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4744\u001b[39m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4745\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4746\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4747\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:4858\u001b[39m, in \u001b[36m_quantile\u001b[39m\u001b[34m(arr, quantiles, axis, method, out, weights)\u001b[39m\n\u001b[32m   4853\u001b[39m previous_indexes, next_indexes = _get_indexes(arr,\n\u001b[32m   4854\u001b[39m                                               virtual_indexes,\n\u001b[32m   4855\u001b[39m                                               values_count)\n\u001b[32m   4856\u001b[39m \u001b[38;5;66;03m# --- Sorting\u001b[39;00m\n\u001b[32m   4857\u001b[39m arr.partition(\n\u001b[32m-> \u001b[39m\u001b[32m4858\u001b[39m     \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4859\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprevious_indexes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4860\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mnext_indexes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4861\u001b[39m \u001b[43m                              \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   4862\u001b[39m     axis=\u001b[32m0\u001b[39m)\n\u001b[32m   4863\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m supports_nans:\n\u001b[32m   4864\u001b[39m     slices_having_nans = np.isnan(arr[-\u001b[32m1\u001b[39m, ...])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_arraysetops_impl.py:294\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan, sorted)\u001b[39m\n\u001b[32m    292\u001b[39m ar = np.asanyarray(ar)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_arraysetops_impl.py:373\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis, sorted)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (hash_unique := _unique_hash(ar_)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msorted\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m         \u001b[43mhash_unique\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# We wrap the result back in case it was a subclass of numpy.ndarray.\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (conv.wrap(hash_unique),)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run inference on all available datasets\n",
    "submission_files = {}\n",
    "\n",
    "scaler_path = \"../models/cnn_scaler.pkl\"\n",
    "scaler = None\n",
    "\n",
    "if os.path.exists(scaler_path):\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    print(f\"✅ Loaded fitted scaler from: {scaler_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  No saved scaler found at {scaler_path}\")\n",
    "    print(\"   Using raw features (may reduce performance)\")\n",
    "    print(\"   Consider saving the scaler during training for better results\")\n",
    "\n",
    "for dataset_name, dataset_path in available_datasets:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🚀 Processing {dataset_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Run inference pipeline\n",
    "        submission_df = run_inference_pipeline(dataset_name, dataset_path, scaler=scaler)\n",
    "        submission_files[dataset_name] = submission_df\n",
    "        \n",
    "        print(f\"✅ Successfully processed {dataset_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n🎉 Inference completed!\")\n",
    "print(f\"📁 Generated {len(submission_files)} submission files:\")\n",
    "for dataset_name in submission_files.keys():\n",
    "    print(f\"   - ../predictions/{dataset_name}_predictions_cnn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c6656",
   "metadata": {},
   "source": [
    "# 6. Optional: Single Dataset Inference\n",
    "\n",
    "Use this cell to run inference on a specific dataset if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run inference on a specific dataset\n",
    "# dataset_path = \"../data/dataset0.json.gz\"\n",
    "# output_path = run_inference_on_dataset(model, dataset_path)\n",
    "# print(f\"Predictions saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
