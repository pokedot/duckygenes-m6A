{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4e164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 11:08:58.988748: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-12 11:09:00.741543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-12 11:09:04.975655: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb3126",
   "metadata": {},
   "source": [
    "# 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77019687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading trained CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 11:09:06.927053: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded model from: ../models/final_cnn_model.keras\n",
      "\n",
      "📊 Model Architecture:\n",
      "  Input shape: (None, 3, 57)\n",
      "  Output shape: (None, 1)\n",
      "  Total parameters: 69,089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 26 variables whereas the saved optimizer has 50 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m22,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m24,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m4,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">137,348</span> (536.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m137,348\u001b[0m (536.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,257</span> (266.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m68,257\u001b[0m (266.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> (3.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m832\u001b[0m (3.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,259</span> (266.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m68,259\u001b[0m (266.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained CNN model\n",
    "model_path = \"../models/final_cnn_model.keras\"\n",
    "legacy_model_path = \"../models/final_cnn_model_legacy.h5\"\n",
    "\n",
    "print(\"🔄 Loading trained CNN model...\")\n",
    "\n",
    "try:\n",
    "    # Try to load modern Keras format first\n",
    "    if os.path.exists(model_path):\n",
    "        model = keras.models.load_model(model_path)\n",
    "        print(f\"✅ Successfully loaded model from: {model_path}\")\n",
    "    elif os.path.exists(legacy_model_path):\n",
    "        model = keras.models.load_model(legacy_model_path)\n",
    "        print(f\"✅ Successfully loaded legacy model from: {legacy_model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No trained model found!\")\n",
    "        \n",
    "    # Display model architecture\n",
    "    print(f\"\\n📊 Model Architecture:\")\n",
    "    print(f\"  Input shape: {model.input_shape}\")\n",
    "    print(f\"  Output shape: {model.output_shape}\")\n",
    "    print(f\"  Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Show model summary\n",
    "    model.summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Make sure you have trained the model first using cnn.ipynb\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221501f",
   "metadata": {},
   "source": [
    "# 2. Define Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4287e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction functions (same as training)\n",
    "def flatten_deep_data(file_path, estimated_rows=11000000):\n",
    "    \"\"\"\n",
    "    Ultra-optimized data loading with batch processing\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 10000\n",
    "    GROWTH_FACTOR = 1.5\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    capacity = estimated_rows\n",
    "    transcript_ids = np.empty(capacity, dtype=object)\n",
    "    positions = np.empty(capacity, dtype=np.int32)\n",
    "    seq = np.empty(capacity, dtype=object)\n",
    "    feature_arrays = [np.empty(capacity, dtype=np.float32) for _ in range(9)]\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    # Temporary batch storage\n",
    "    batch_transcript_ids = []\n",
    "    batch_positions = []\n",
    "    batch_seq = []\n",
    "    batch_features = [[] for _ in range(9)]\n",
    "    \n",
    "    def flush_batch():\n",
    "        \"\"\"Flush batch to main arrays\"\"\"\n",
    "        nonlocal idx, capacity\n",
    "        \n",
    "        batch_size = len(batch_transcript_ids)\n",
    "        if batch_size == 0:\n",
    "            return\n",
    "        \n",
    "        # Resize if needed\n",
    "        while idx + batch_size > capacity:\n",
    "            new_capacity = int(capacity * GROWTH_FACTOR)\n",
    "            transcript_ids.resize(new_capacity, refcheck=False)\n",
    "            positions.resize(new_capacity, refcheck=False)\n",
    "            seq.resize(new_capacity, refcheck=False)\n",
    "            for i in range(9):\n",
    "                feature_arrays[i].resize(new_capacity, refcheck=False)\n",
    "            capacity = new_capacity\n",
    "        \n",
    "        # Bulk assignment\n",
    "        transcript_ids[idx:idx+batch_size] = batch_transcript_ids\n",
    "        positions[idx:idx+batch_size] = batch_positions\n",
    "        seq[idx:idx+batch_size] = batch_seq\n",
    "        for i in range(9):\n",
    "            feature_arrays[i][idx:idx+batch_size] = batch_features[i]\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "        # Clear batch\n",
    "        batch_transcript_ids.clear()\n",
    "        batch_positions.clear()\n",
    "        batch_seq.clear()\n",
    "        for lst in batch_features:\n",
    "            lst.clear()\n",
    "    \n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            for transcript_id, positions_dict in data.items():\n",
    "                for transcript_position, sequences in positions_dict.items():\n",
    "                    pos_int = int(transcript_position)\n",
    "                    for sequence, feature_list in sequences.items():\n",
    "                        for features in feature_list:\n",
    "                            # Add to batch\n",
    "                            batch_transcript_ids.append(transcript_id)\n",
    "                            batch_positions.append(pos_int)\n",
    "                            batch_seq.append(sequence)\n",
    "                            for i, val in enumerate(features):\n",
    "                                batch_features[i].append(val)\n",
    "                            \n",
    "                            # Flush when batch is full\n",
    "                            if len(batch_transcript_ids) >= BATCH_SIZE:\n",
    "                                flush_batch()\n",
    "                                \n",
    "                                if idx % 100000 == 0:\n",
    "                                    print(f\"Processed {idx:,} rows...\", end='\\r')\n",
    "    \n",
    "    # Flush remaining batch\n",
    "    flush_batch()\n",
    "    \n",
    "    print(f\"\\nCreating DataFrame with {idx:,} rows...\")\n",
    "    \n",
    "    # Trim and create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'transcript_id': transcript_ids[:idx],\n",
    "        'transcript_position': positions[:idx],\n",
    "        'sequence': seq[:idx],\n",
    "        'dwell_-1': feature_arrays[0][:idx],\n",
    "        'std_-1': feature_arrays[1][:idx],\n",
    "        'mean_-1': feature_arrays[2][:idx],\n",
    "        'dwell_0': feature_arrays[3][:idx],\n",
    "        'std_0': feature_arrays[4][:idx],\n",
    "        'mean_0': feature_arrays[5][:idx],\n",
    "        'dwell_+1': feature_arrays[6][:idx],\n",
    "        'std_+1': feature_arrays[7][:idx],\n",
    "        'mean_+1': feature_arrays[8][:idx],\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_cnn_features(group):\n",
    "    \"\"\"Create CNN features with sliding window 5-mer approach (same as training)\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Define positions and their feature types\n",
    "    positions = ['-1', '0', '+1']\n",
    "    feature_types = ['dwell', 'std', 'mean']\n",
    "    \n",
    "    # 1. ORIGINAL SIGNAL FEATURES (preserving spatial organization)\n",
    "    for pos in positions:\n",
    "        for feat_type in feature_types:\n",
    "            col = f'{feat_type}_{pos}'\n",
    "            # Create all 9 statistics for this feature at this position\n",
    "            features[f'{col}_mean'] = group[col].mean()\n",
    "            features[f'{col}_median'] = group[col].median()\n",
    "            features[f'{col}_std'] = group[col].std()\n",
    "            features[f'{col}_iqr'] = group[col].quantile(0.75) - group[col].quantile(0.25)\n",
    "            features[f'{col}_skew'] = group[col].skew()\n",
    "            features[f'{col}_min'] = group[col].min()\n",
    "            features[f'{col}_max'] = group[col].max()\n",
    "            features[f'{col}_q25'] = group[col].quantile(0.25)\n",
    "            features[f'{col}_q75'] = group[col].quantile(0.75)\n",
    "    \n",
    "    # 2. SEQUENCE ONE-HOT ENCODING (sliding window 5-mers)\n",
    "    consensus_sequence = group['sequence'].iloc[0]  # 7-mer sequence\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    \n",
    "    # Define 5-mer windows for each CNN position\n",
    "    cnn_position_windows = {\n",
    "        '-1': [0, 1, 2, 3, 4],  # Characters 1-5 (indices 0-4)\n",
    "        '0':  [1, 2, 3, 4, 5],  # Characters 2-6 (indices 1-5)\n",
    "        '+1': [2, 3, 4, 5, 6]   # Characters 3-7 (indices 2-6)\n",
    "    }\n",
    "    \n",
    "    # For each CNN position, create one-hot features for the 5-mer window\n",
    "    for pos in positions:\n",
    "        seq_indices = cnn_position_windows[pos]\n",
    "        \n",
    "        for seq_idx in seq_indices:\n",
    "            if seq_idx < len(consensus_sequence):\n",
    "                nucleotide = consensus_sequence[seq_idx]\n",
    "                seq_pos_label = seq_idx - 3  # Convert to relative position (-3 to +3)\n",
    "                \n",
    "                # Create one-hot encoding for this sequence position\n",
    "                for nt in nucleotides:\n",
    "                    features[f'seq_pos{seq_pos_label}_{nt}_{pos}'] = 1 if nucleotide == nt else 0\n",
    "    \n",
    "    # 3. ADDITIONAL SEQUENCE FEATURES (5-mer composition)\n",
    "    for pos in positions:\n",
    "        seq_indices = cnn_position_windows[pos]\n",
    "        \n",
    "        # Count nucleotides for this CNN position's 5-mer window\n",
    "        pos_sequence = ''.join([consensus_sequence[i] for i in seq_indices if i < len(consensus_sequence)])\n",
    "        \n",
    "        for nt in nucleotides:\n",
    "            features[f'nt_count_{nt}_{pos}'] = pos_sequence.count(nt)\n",
    "            features[f'nt_freq_{nt}_{pos}'] = pos_sequence.count(nt) / len(pos_sequence) if pos_sequence else 0\n",
    "        \n",
    "        # Purine/Pyrimidine for this position's 5-mer\n",
    "        purines = sum(1 for n in pos_sequence if n in ['A', 'G'])\n",
    "        features[f'purine_count_{pos}'] = purines\n",
    "        features[f'purine_freq_{pos}'] = purines / len(pos_sequence) if pos_sequence else 0\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "print(\"✅ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b201b6",
   "metadata": {},
   "source": [
    "# 3. Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5117a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data preparation functions defined\n"
     ]
    }
   ],
   "source": [
    "def prepare_inference_data(features_df, scaler=None):\n",
    "    \"\"\"Prepare data for CNN inference (no labels needed)\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Preparing inference data...\")\n",
    "    print(f\"Total features available: {features_df.shape[1]}\")\n",
    "    \n",
    "    # Organize features by position for proper CNN spatial structure\n",
    "    feature_names = features_df.columns\n",
    "    \n",
    "    # Group ALL features by position (-1, 0, +1)\n",
    "    pos_minus1_features = [f for f in feature_names if '_-1' in f]\n",
    "    pos_0_features = [f for f in feature_names if '_0' in f and '_-1' not in f and '_+1' not in f]  \n",
    "    pos_plus1_features = [f for f in feature_names if '_+1' in f]\n",
    "    \n",
    "    print(f\"Features per position: -1={len(pos_minus1_features)}, 0={len(pos_0_features)}, +1={len(pos_plus1_features)}\")\n",
    "    \n",
    "    # Check that features are evenly distributed across positions\n",
    "    if len(pos_minus1_features) != len(pos_0_features) or len(pos_0_features) != len(pos_plus1_features):\n",
    "        print(f\"⚠️  Warning: Uneven feature distribution across positions!\")\n",
    "        print(f\"Position -1 features: {len(pos_minus1_features)}\")\n",
    "        print(f\"Position 0 features: {len(pos_0_features)}\")  \n",
    "        print(f\"Position +1 features: {len(pos_plus1_features)}\")\n",
    "    \n",
    "    features_per_position = len(pos_minus1_features)\n",
    "    \n",
    "    # Create properly ordered feature matrix\n",
    "    ordered_features = pos_minus1_features + pos_0_features + pos_plus1_features\n",
    "    feature_matrix = features_df[ordered_features].values\n",
    "    \n",
    "    # Reshape to (N_samples, 3_positions, features_per_position)\n",
    "    feature_matrix = feature_matrix.reshape(-1, 3, features_per_position)\n",
    "    print(f\"Reshaped feature matrix: {feature_matrix.shape} (samples, positions, features_per_position)\")\n",
    "    \n",
    "    # Scale features if scaler provided\n",
    "    if scaler is not None:\n",
    "        # Reshape to 2D, scale, reshape back\n",
    "        original_shape = feature_matrix.shape\n",
    "        feature_matrix_scaled = scaler.transform(feature_matrix.reshape(-1, features_per_position)).reshape(original_shape)\n",
    "        print(f\"✅ Features scaled using provided scaler\")\n",
    "        return feature_matrix_scaled\n",
    "    else:\n",
    "        print(\"⚠️  No scaler provided - using raw features (may affect performance)\")\n",
    "        return feature_matrix\n",
    "\n",
    "def create_submission_file(transcript_ids, positions, predictions, output_path):\n",
    "    \"\"\"Create submission file with predictions\"\"\"\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'transcript_id': transcript_ids,\n",
    "        'transcript_position': positions,\n",
    "        'score': predictions.flatten()\n",
    "    })\n",
    "    \n",
    "    # Create predictions directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Submission file saved: {output_path}\")\n",
    "    print(f\"   Shape: {submission_df.shape}\")\n",
    "    print(f\"   Sample predictions:\")\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📊 Prediction Statistics:\")\n",
    "    print(f\"   Mean score: {submission_df['score'].mean():.4f}\")\n",
    "    print(f\"   Std score: {submission_df['score'].std():.4f}\")\n",
    "    print(f\"   Min score: {submission_df['score'].min():.4f}\")\n",
    "    print(f\"   Max score: {submission_df['score'].max():.4f}\")\n",
    "    print(f\"   Predicted positives (>0.5): {(submission_df['score'] > 0.5).sum()}\")\n",
    "    print(f\"   Predicted positives (%): {(submission_df['score'] > 0.5).mean() * 100:.2f}%\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "print(\"✅ Data preparation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98603fe6",
   "metadata": {},
   "source": [
    "# 4. Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387e8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "def run_inference_pipeline(dataset_name, dataset_file, scaler=None):\n",
    "    \"\"\"\n",
    "    Complete inference pipeline for a dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name for output file (e.g., 'dataset0', 'dataset1')\n",
    "        dataset_file: Path to dataset JSON.gz file\n",
    "        scaler: Fitted scaler from training (optional but recommended)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Starting inference pipeline for {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load and process data\n",
    "    print(f\"📁 Loading data from {dataset_file}...\")\n",
    "    df = flatten_deep_data(dataset_file)\n",
    "    \n",
    "    print(f\"📊 Dataset statistics:\")\n",
    "    print(f\"   Total rows: {len(df):,}\")\n",
    "    print(f\"   Unique positions: {df.groupby(['transcript_id', 'transcript_position']).ngroups:,}\")\n",
    "    \n",
    "    # 2. Extract features\n",
    "    print(f\"\\n🔧 Extracting CNN features...\")\n",
    "    tqdm.pandas()\n",
    "    cnn_features = df.groupby(['transcript_id', 'transcript_position']).progress_apply(\n",
    "        create_cnn_features, include_groups=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Feature extraction complete:\")\n",
    "    print(f\"   Feature matrix shape: {cnn_features.shape}\")\n",
    "    print(f\"   Features per position: {cnn_features.shape[1] // 3}\")\n",
    "    \n",
    "    # 3. Prepare data for inference\n",
    "    X_inference = prepare_inference_data(cnn_features, scaler=scaler)\n",
    "    \n",
    "    # 4. Generate predictions\n",
    "    print(f\"\\n🤖 Generating predictions...\")\n",
    "    predictions = model.predict(X_inference, batch_size=256, verbose=1)\n",
    "    \n",
    "    # 5. Prepare output data\n",
    "    transcript_ids = [idx[0] for idx in cnn_features.index]\n",
    "    positions = [idx[1] for idx in cnn_features.index]\n",
    "    \n",
    "    # 6. Create submission file\n",
    "    output_path = f\"../predictions/{dataset_name}_predictions_cnn.csv\"\n",
    "    submission_df = create_submission_file(transcript_ids, positions, predictions, output_path)\n",
    "    \n",
    "    print(f\"\\n✅ Inference pipeline completed for {dataset_name}\")\n",
    "    return submission_df\n",
    "\n",
    "print(\"✅ Inference pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3517d",
   "metadata": {},
   "source": [
    "# 5. Run Inference on Available Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8d93b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 3 datasets:\n",
      "   - dataset0: ../data/dataset0.json.gz\n",
      "   - dataset1: ../data/dataset1.json.gz\n",
      "   - dataset2: ../data/dataset2.json.gz\n",
      "\n",
      "✅ Ready to run inference on 3 datasets\n"
     ]
    }
   ],
   "source": [
    "# Discover available datasets\n",
    "data_dir = \"../data/\"\n",
    "available_datasets = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.startswith(\"dataset\") and file.endswith(\".json.gz\"):\n",
    "        dataset_name = file.replace(\".json.gz\", \"\")\n",
    "        dataset_path = os.path.join(data_dir, file)\n",
    "        available_datasets.append((dataset_name, dataset_path))\n",
    "\n",
    "print(f\"🔍 Found {len(available_datasets)} datasets:\")\n",
    "for name, path in available_datasets:\n",
    "    print(f\"   - {name}: {path}\")\n",
    "\n",
    "if not available_datasets:\n",
    "    print(\"⚠️  No datasets found in ../data/ directory\")\n",
    "    print(\"   Make sure dataset files are named like: dataset0.json.gz, dataset1.json.gz, etc.\")\n",
    "else:\n",
    "    print(f\"\\n✅ Ready to run inference on {len(available_datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802d4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded fitted scaler from: ../models/cnn_scaler.pkl\n",
      "\n",
      "================================================================================\n",
      "🚀 Processing dataset0\n",
      "================================================================================\n",
      "\n",
      "🚀 Starting inference pipeline for dataset0\n",
      "============================================================\n",
      "📁 Loading data from ../data/dataset0.json.gz...\n",
      "Processed 11,000,000 rows...\n",
      "Creating DataFrame with 11,027,106 rows...\n",
      "📊 Dataset statistics:\n",
      "   Total rows: 11,027,106\n",
      "   Unique positions: 121,838\n",
      "\n",
      "🔧 Extracting CNN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121838/121838 [46:58<00:00, 43.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction complete:\n",
      "   Feature matrix shape: (121838, 171)\n",
      "   Features per position: 57\n",
      "🔄 Preparing inference data...\n",
      "Total features available: 171\n",
      "Features per position: -1=57, 0=57, +1=57\n",
      "Reshaped feature matrix: (121838, 3, 57) (samples, positions, features_per_position)\n",
      "✅ Features scaled using provided scaler\n",
      "\n",
      "🤖 Generating predictions...\n",
      "\u001b[1m476/476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "✅ Submission file saved: ../predictions/dataset0_predictions_cnn.csv\n",
      "   Shape: (121838, 3)\n",
      "   Sample predictions:\n",
      "     transcript_id  transcript_position     score\n",
      "0  ENST00000000233                  244  0.002405\n",
      "1  ENST00000000233                  261  0.285987\n",
      "2  ENST00000000233                  316  0.092857\n",
      "3  ENST00000000233                  332  0.093060\n",
      "4  ENST00000000233                  368  0.002643\n",
      "\n",
      "📊 Prediction Statistics:\n",
      "   Mean score: 0.1734\n",
      "   Std score: 0.2842\n",
      "   Min score: 0.0000\n",
      "   Max score: 0.9971\n",
      "   Predicted positives (>0.5): 18196\n",
      "   Predicted positives (%): 14.93%\n",
      "\n",
      "✅ Inference pipeline completed for dataset0\n",
      "✅ Successfully processed dataset0\n",
      "\n",
      "================================================================================\n",
      "🚀 Processing dataset1\n",
      "================================================================================\n",
      "\n",
      "🚀 Starting inference pipeline for dataset1\n",
      "============================================================\n",
      "📁 Loading data from ../data/dataset1.json.gz...\n",
      "Processed 7,900,000 rows...\n",
      "Creating DataFrame with 7,907,952 rows...\n",
      "📊 Dataset statistics:\n",
      "   Total rows: 7,907,952\n",
      "   Unique positions: 90,810\n",
      "\n",
      "🔧 Extracting CNN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90810/90810 [33:58<00:00, 44.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction complete:\n",
      "   Feature matrix shape: (90810, 171)\n",
      "   Features per position: 57\n",
      "🔄 Preparing inference data...\n",
      "Total features available: 171\n",
      "Features per position: -1=57, 0=57, +1=57\n",
      "Reshaped feature matrix: (90810, 3, 57) (samples, positions, features_per_position)\n",
      "✅ Features scaled using provided scaler\n",
      "\n",
      "🤖 Generating predictions...\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "✅ Submission file saved: ../predictions/dataset1_predictions_cnn.csv\n",
      "   Shape: (90810, 3)\n",
      "   Sample predictions:\n",
      "     transcript_id  transcript_position     score\n",
      "0  ENST00000000233                  244  0.012998\n",
      "1  ENST00000000233                  261  0.345986\n",
      "2  ENST00000000233                  316  0.088679\n",
      "3  ENST00000000233                  332  0.360784\n",
      "4  ENST00000000233                  368  0.002382\n",
      "\n",
      "📊 Prediction Statistics:\n",
      "   Mean score: 0.1869\n",
      "   Std score: 0.2924\n",
      "   Min score: 0.0000\n",
      "   Max score: 0.9982\n",
      "   Predicted positives (>0.5): 14861\n",
      "   Predicted positives (%): 16.36%\n",
      "\n",
      "✅ Inference pipeline completed for dataset1\n",
      "✅ Successfully processed dataset1\n",
      "\n",
      "================================================================================\n",
      "🚀 Processing dataset2\n",
      "================================================================================\n",
      "\n",
      "🚀 Starting inference pipeline for dataset2\n",
      "============================================================\n",
      "📁 Loading data from ../data/dataset2.json.gz...\n",
      "Processed 1,100,000 rows...\n",
      "Creating DataFrame with 1,171,940 rows...\n",
      "📊 Dataset statistics:\n",
      "   Total rows: 1,171,940\n",
      "   Unique positions: 1,323\n",
      "\n",
      "🔧 Extracting CNN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1323/1323 [00:30<00:00, 43.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction complete:\n",
      "   Feature matrix shape: (1323, 171)\n",
      "   Features per position: 57\n",
      "🔄 Preparing inference data...\n",
      "Total features available: 171\n",
      "Features per position: -1=57, 0=57, +1=57\n",
      "Reshaped feature matrix: (1323, 3, 57) (samples, positions, features_per_position)\n",
      "✅ Features scaled using provided scaler\n",
      "\n",
      "🤖 Generating predictions...\n",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "✅ Submission file saved: ../predictions/dataset2_predictions_cnn.csv\n",
      "   Shape: (1323, 3)\n",
      "   Sample predictions:\n",
      "  transcript_id  transcript_position     score\n",
      "0       tx_id_0                    0  0.005138\n",
      "1       tx_id_0                   10  0.882508\n",
      "2       tx_id_0                   20  0.984228\n",
      "3       tx_id_0                   30  0.991710\n",
      "4       tx_id_0                   40  0.751884\n",
      "\n",
      "📊 Prediction Statistics:\n",
      "   Mean score: 0.5070\n",
      "   Std score: 0.3924\n",
      "   Min score: 0.0000\n",
      "   Max score: 0.9934\n",
      "   Predicted positives (>0.5): 692\n",
      "   Predicted positives (%): 52.31%\n",
      "\n",
      "✅ Inference pipeline completed for dataset2\n",
      "✅ Successfully processed dataset2\n",
      "\n",
      "🎉 Inference completed!\n",
      "📁 Generated 3 submission files:\n",
      "   - ../predictions/dataset0_predictions_cnn.csv\n",
      "   - ../predictions/dataset1_predictions_cnn.csv\n",
      "   - ../predictions/dataset2_predictions_cnn.csv\n"
     ]
    }
   ],
   "source": [
    "# Run inference on all available datasets\n",
    "submission_files = {}\n",
    "\n",
    "scaler_path = \"../models/cnn_scaler.pkl\"\n",
    "scaler = None\n",
    "\n",
    "if os.path.exists(scaler_path):\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    print(f\"✅ Loaded fitted scaler from: {scaler_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  No saved scaler found at {scaler_path}\")\n",
    "    print(\"   Using raw features (may reduce performance)\")\n",
    "    print(\"   Consider saving the scaler during training for better results\")\n",
    "\n",
    "for dataset_name, dataset_path in available_datasets:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🚀 Processing {dataset_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Run inference pipeline\n",
    "        submission_df = run_inference_pipeline(dataset_name, dataset_path, scaler=scaler)\n",
    "        submission_files[dataset_name] = submission_df\n",
    "        \n",
    "        print(f\"✅ Successfully processed {dataset_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n🎉 Inference completed!\")\n",
    "print(f\"📁 Generated {len(submission_files)} submission files:\")\n",
    "for dataset_name in submission_files.keys():\n",
    "    print(f\"   - ../predictions/{dataset_name}_predictions_cnn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
